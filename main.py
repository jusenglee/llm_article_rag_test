import logging, nest_asyncio
from fastapi import FastAPI, Request, Form
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from query_pipeline import (
    build_rag_objects, triton_infer, expand_query_kor, expand_variants,
    dense_retrieve_hybrid, rag_gate_decision, build_chat_prompt,
    keyword_boost, rrf_rerank, dedup_by_doc, build_context_and_refs,
    trim_context_to_budget, build_rag_prompt, decide_rag_needed
)

# ---------------------------
# ì´ˆê¸° ì„¤ì •
# ---------------------------
nest_asyncio.apply()  # âœ… JupyterLab í˜¸í™˜ì„ ìœ„í•´ ì´ë²¤íŠ¸ ë£¨í”„ ì¤‘ì²© í—ˆìš©

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger("uvicorn")  # uvicorn stdoutê³¼ ë™ì¼í•œ ì±„ë„

logger.info("ğŸš€ Initializing RAG components...")
qdr, emb, retriever = build_rag_objects()
logger.info("âœ… RAG pipeline ready.\n")

app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# ---------------------------
# ë¼ìš°íŠ¸
# ---------------------------
@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/query")
async def query_rag(request: Request, question: str = Form(...)):
    logs = []  # ì‚¬ìš©ìì—ê²Œ ë³´ë‚¼ ë¡œê·¸ ëª¨ìŒ

    def log(msg):
        logger.info(msg)
        logs.append(msg)

    log(f"\nì§ˆë¬¸ > {question}")
    need_rag = decide_rag_needed(question)
    log(f"ğŸ§­ LLM íŒë‹¨ ê²°ê³¼: {'RAG ê²€ìƒ‰ ìˆ˜í–‰' if need_rag else 'ì¼ë°˜ ëŒ€í™”'}")

    expanded_text, kw_list = expand_query_kor(question)
    keywords = expand_variants(kw_list)
    log(f"ğŸ”‘ í™•ì¥ í‚¤ì›Œë“œ: {keywords}")

    hits = dense_retrieve_hybrid(qdr, emb, expanded_text, keywords)
    log(f"ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜(max 20): {len(hits)}")
    ok, gate_msg = rag_gate_decision(question, hits, kw_list, need_rag)
    log(gate_msg)

    if not ok:
        log("ğŸ¤– ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¡œ ì „í™˜")
        prompt = build_chat_prompt(question)
        answer = triton_infer(prompt, stream=True)
        log(f"\nğŸ“˜ ë‹µë³€:\n{answer.strip()}\n" + "-"*80)
        return JSONResponse({"mode": "chat", "answer": answer.strip(), "logs": logs})

    log("âœ… ê²Œì´íŠ¸ íŒë‹¨ ê²°ê³¼: RAG ê²€ìƒ‰/ì‘ë‹µ ìˆ˜í–‰")
    boost_map = keyword_boost(hits, kw_list)
    reranked = rrf_rerank(hits, boost_map)
    final_hits = dedup_by_doc(reranked)
    ctx, refs = build_context_and_refs(final_hits)
    ctx = trim_context_to_budget(ctx)
    log(f"ğŸ” Retrieved top-{len(final_hits)} (after RRF+dedup).")

    rag_prompt = build_rag_prompt(ctx, question, refs)
    log("âš¡ LLM generating response...")
    answer = triton_infer(rag_prompt, stream=True)
    log(f"\nğŸ“˜ ë‹µë³€:\n{answer.strip()}\n" + "-"*80)

    return JSONResponse({"mode": "rag", "answer": answer.strip(), "refs": refs, "logs": logs})


# ---------------------------
# Jupyter / CLI ì‹¤í–‰ ëª¨ë‘ ì§€ì›
# ---------------------------
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8082, reload=False)
