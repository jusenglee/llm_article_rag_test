# -*- coding: utf-8 -*-
import logging, nest_asyncio, time, traceback
from fastapi import FastAPI, Request, Form
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from starlette.responses import StreamingResponse

from query_pipeline import (
    build_rag_objects, triton_infer, expand_query_kor, expand_variants,
    dense_retrieve_hybrid, rag_gate_decision, build_chat_prompt,
    keyword_boost, rrf_rerank, dedup_by_doc, build_context_and_refs,
    trim_context_to_budget, build_rag_prompt, decide_rag_needed, tokenizer
)

# ---------------------------
# ì´ˆê¸° ì„¤ì •
# ---------------------------
nest_asyncio.apply()

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger("uvicorn")

app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# ì „ì—­ ê°ì²´ (ë©€í‹°ì›Œì»¤ì¼ ë• í”„ë¡œì„¸ìŠ¤ë³„ ì´ˆê¸°í™”)
qdr = emb = retriever = None

@app.on_event("startup")
async def init_rag():
    global qdr, emb, retriever
    logger.info("ğŸš€ Initializing RAG components...")
    qdr, emb, retriever = build_rag_objects()
    # Warmup: GPU/í† í¬ë‚˜ì´ì €/JIT lazy init ë¹„ìš© ì œê±°
    try:
        from query_pipeline import tokenizer
        _ = tokenizer.encode("warmup")
        _ = emb.get_text_embedding("warmup")
    except Exception as e:
        logger.info(f"Warmup skipped: {e}")
    logger.info("âœ… RAG pipeline ready.\n")

# ---------------------------
# ë¼ìš°íŠ¸
# ---------------------------
@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

# ---------------------------
# SSE STREAM
# ---------------------------
@app.get("/query/stream")
async def query_stream(question: str):

    async def event_gen():
        try:
            import asyncio
            yield "data: [STEP 0] ì§ˆë¬¸ ìˆ˜ì‹ \n\n"
            await asyncio.sleep(0)  # ì²« flush

            # STEP 1: ê²Œì´íŠ¸ íŒë‹¨
            t0 = time.time()
            need_rag = decide_rag_needed(question)
            t1 = time.time()
            yield f"data: [STEP 1] ê²Œì´íŠ¸={need_rag} (t={t1 - t0:.2f}s)\n\n"

            # ---------------------------------------------------------
            # â˜… ê²Œì´íŠ¸=False â†’ RAG ì „ì²´ ìŠ¤í‚µ (ì¼ë°˜ ëŒ€í™”ë§Œ)
            # ---------------------------------------------------------
            if not need_rag:
                yield "data: [STEP 2] RAG ìŠ¤í‚µ â†’ ì¼ë°˜ ëŒ€í™” ì§„í–‰\n\n"
                prompt = build_chat_prompt(question)

                yield "data: [STEP 3] LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (chat)\n\n"

                # í† í° ë‹¨ìœ„ë¡œë§Œ ë³´ë‚´ê³ , decoded ë”°ë¡œ ì•ˆ ë³´ëƒ„
                for chunk in triton_infer(prompt, stream=True):
                    if chunk == "" or chunk is None:
                        yield "data: [END]\n\n"
                        return
                    yield f"data: {chunk}\n\n"

            # ---------------------------------------------------------
            # â˜… need_rag == True â†’ RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ ìˆ˜í–‰
            # ---------------------------------------------------------
            yield "data: [STEP 2] í™•ì¥/ê²€ìƒ‰ ì‹œì‘ (RAG)\n\n"

            expanded_text, kw_list = expand_query_kor(question)
            keywords = expand_variants(kw_list)
            yield f"data: [STEP 2] í™•ì¥ í‚¤ì›Œë“œ={keywords}\n\n"

            t2 = time.time()
            hits = dense_retrieve_hybrid(qdr, emb, expanded_text, keywords)
            t3 = time.time()
            yield f"data: [STEP 3] hits={len(hits)} (t={t3 - t2:.2f}s)\n\n"

            ok, _ = rag_gate_decision(question, hits, kw_list, need_rag)

            if not ok:
                yield "data: [STEP 3b] ê²€ìƒ‰ ìŠ¤ì½”ì–´ ë‚®ìŒ â†’ Chatìœ¼ë¡œ ì „í™˜\n\n"
                prompt = build_chat_prompt(question)
                mode = "chat"
            else:
                mode = "rag"
                yield "data: [STEP 4] ë¬¸ë§¥ êµ¬ì„± ì‹œì‘\n\n"
                boost_map = keyword_boost(hits, kw_list)
                reranked = rrf_rerank(hits, boost_map)
                final_hits = dedup_by_doc(reranked)
                ctx, refs = build_context_and_refs(final_hits)
                ctx = trim_context_to_budget(ctx)
                prompt = build_rag_prompt(ctx, question, refs)

            yield f"data: [STEP 4] ëª¨ë“œ={mode}\n\n"

            # STEP 5: LLM ìŠ¤íŠ¸ë¦¬ë°
            yield "data: [STEP 5] LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘\n\n"
            for chunk in triton_infer(prompt, stream=True):
                if chunk:
                    yield f"data: {chunk}\n\n"

            yield "data: [END]\n\n"

        except Exception as e:
            err = f"{type(e).__name__}: {e}"
            traceback.print_exc()
            yield f"data: âš ï¸ ì˜¤ë¥˜: {err}\n\n"
            yield "data: [END]\n\n"

    return StreamingResponse(
        event_gen(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
            "Transfer-Encoding": "chunked",
        }
    )
# ---------------------------
# ë¡œì»¬ ì‹¤í–‰
# ---------------------------
if __name__ == "__main__":
    import uvicorn
    # ìš´ì˜ì—ì„œëŠ” --workers 1 ê¶Œì¥ (ì „ì—­ ì»¤ë„¥ì…˜ ì¬ì‚¬ìš© ë° ë””ë²„ê¹… í¸ì˜)
    uvicorn.run(app, host="0.0.0.0", port=8082, reload=False)
