# -*- coding: utf-8 -*-
import logging, nest_asyncio, time, traceback
from fastapi import FastAPI, Request, Form
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from starlette.responses import StreamingResponse

from query_pipeline import (
    build_rag_objects, triton_infer, expand_query_kor, expand_variants,
    dense_retrieve_hybrid, rag_gate_decision, build_chat_prompt,
    keyword_boost, rrf_rerank, dedup_by_doc, build_context_and_refs,
    trim_context_to_budget, build_rag_prompt, decide_rag_needed, tokenizer
)

# ---------------------------
# ì´ˆê¸° ì„¤ì •
# ---------------------------
nest_asyncio.apply()

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger("uvicorn")

app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# ì „ì—­ ê°ì²´ (ë©€í‹°ì›Œì»¤ì¼ ë• í”„ë¡œì„¸ìŠ¤ë³„ ì´ˆê¸°í™”)
qdr = emb = retriever = None

@app.on_event("startup")
async def init_rag():
    global qdr, emb, retriever
    logger.info("ğŸš€ Initializing RAG components...")
    qdr, emb, retriever = build_rag_objects()
    # Warmup: GPU/í† í¬ë‚˜ì´ì €/JIT lazy init ë¹„ìš© ì œê±°
    try:
        from query_pipeline import tokenizer
        _ = tokenizer.encode("warmup")
        _ = emb.get_text_embedding("warmup")
    except Exception as e:
        logger.info(f"Warmup skipped: {e}")
    logger.info("âœ… RAG pipeline ready.\n")

# ---------------------------
# ë¼ìš°íŠ¸
# ---------------------------
@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/query")
async def query_rag(request: Request, question: str = Form(...)):
    logs = []
    t0 = time.time()

    def log(msg):
        logger.info(msg); logs.append(msg)

    try:
        log(f"\nì§ˆë¬¸ > {question}")

        # --- ê²Œì´íŠ¸ íŒë‹¨ ---
        t1 = time.time()
        need_rag = decide_rag_needed(question)  # ë™ê¸°(ë¹„ìŠ¤íŠ¸ë¦¼)
        t2 = time.time()

        log(f"ğŸ§­ LLM íŒë‹¨ ê²°ê³¼: {'RAG ê²€ìƒ‰ ìˆ˜í–‰' if need_rag else 'ì¼ë°˜ ëŒ€í™”'}  "
            f"(gate={t2 - t1:.2f}s)")

        # --- í™•ì¥/ê²€ìƒ‰ ---
        expanded_text, kw_list = expand_query_kor(question)
        keywords = expand_variants(kw_list)
        log(f"ğŸ”‘ í™•ì¥ í‚¤ì›Œë“œ: {keywords}")

        t3 = time.time()
        hits = dense_retrieve_hybrid(qdr, emb, expanded_text, keywords)
        t4 = time.time()
        log(f"ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜(max 20): {len(hits)}  (search={t4 - t3:.2f}s)")

        ok, gate_msg = rag_gate_decision(question, hits, kw_list, need_rag)
        log(gate_msg)

        # --- ì¼ë°˜ ëŒ€í™” ---
        if not ok:
            prompt = build_chat_prompt(question)
            t5 = time.time()
            full = triton_infer(prompt, stream=False)  # ë¬¸ìì—´(ë¹„ìŠ¤íŠ¸ë¦¼)
            t6 = time.time()
            log(f"âš¡ LLM: {t6 - t5:.2f}s")
            log("\nğŸ“˜ ë‹µë³€:\n" + full.strip() + "\n" + "-"*80)
            return JSONResponse({"mode": "chat", "answer": full.strip(), "logs": logs})

        # --- RAG ---
        log("âœ… ê²Œì´íŠ¸ íŒë‹¨ ê²°ê³¼: RAG ê²€ìƒ‰/ì‘ë‹µ ìˆ˜í–‰")
        boost_map = keyword_boost(hits, kw_list)
        reranked = rrf_rerank(hits, boost_map)
        final_hits = dedup_by_doc(reranked)
        ctx, refs = build_context_and_refs(final_hits)
        ctx = trim_context_to_budget(ctx)
        log(f"ğŸ” Retrieved top-{len(final_hits)} (after RRF+dedup).")

        rag_prompt = build_rag_prompt(ctx, question, refs)
        t7 = time.time()
        full = triton_infer(rag_prompt, stream=False)  # ë¬¸ìì—´(ë¹„ìŠ¤íŠ¸ë¦¼)
        t8 = time.time()
        log(f"âš¡ LLM: {t8 - t7:.2f}s")
        log("\nğŸ“˜ ë‹µë³€:\n" + full.strip() + "\n" + "-"*80)

        return JSONResponse({"mode": "rag", "answer": full.strip(), "refs": refs, "logs": logs})

    except Exception as e:
        traceback.print_exc()
        return JSONResponse({"error": str(e)}, status_code=500)

# ---------------------------
# SSE STREAM
# ---------------------------
@app.get("/query/stream")
async def query_stream(question: str):

    async def event_gen():
        try:
            yield "data: [STEP 0] ì§ˆë¬¸ ìˆ˜ì‹ \n\n"

            # STEP 1: ê²Œì´íŠ¸ íŒë‹¨
            t0 = time.time()
            need_rag = decide_rag_needed(question)
            t1 = time.time()
            yield f"data: [STEP 1] ê²Œì´íŠ¸={need_rag} (t={t1 - t0:.2f}s)\n\n"

            # ---------------------------------------------------------
            # â˜… ê²Œì´íŠ¸=False â†’ RAG ì „ì²´ ìŠ¤í‚µ (ìµœì í™” í•µì‹¬)
            # ---------------------------------------------------------
            if not need_rag:
                yield "data: [STEP 2] RAG ìŠ¤í‚µ â†’ ì¼ë°˜ ëŒ€í™” ì§„í–‰\n\n"
                prompt = build_chat_prompt(question)

                yield "data: [STEP 3] LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (chat)\n\n"
                full_bytes = bytearray()
                for chunk in triton_infer(prompt, stream=True):
                    if not chunk:
                        continue

                    # chunk ìì²´ê°€ bytesì„ â†’ í†µì§¸ë¡œ ì €ì¥
                    if isinstance(chunk, bytes):
                        full_bytes.extend(chunk)
                    else:
                        full_bytes.extend(chunk.encode("utf-8"))

                    # ì¤‘ê°„ì—ëŠ” placeholder ì¶œë ¥ (ì›í•˜ë©´)
                    yield f"data: {chunk}\n\n"

                # ìµœì¢… ë””ì½”ë”©
                decoded = tokenizer.decode(list(full_bytes))

                yield f"data: {decoded}\n\n"
                yield "data: [END]\n\n"

            # ---------------------------------------------------------
            # â˜… need_rag == True â†’ RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ ìˆ˜í–‰
            # ---------------------------------------------------------
            yield "data: [STEP 2] í™•ì¥/ê²€ìƒ‰ ì‹œì‘ (RAG)\n\n"

            expanded_text, kw_list = expand_query_kor(question)
            keywords = expand_variants(kw_list)
            yield f"data: [STEP 2] í™•ì¥ í‚¤ì›Œë“œ={keywords}\n\n"

            t2 = time.time()
            hits = dense_retrieve_hybrid(qdr, emb, expanded_text, keywords)
            t3 = time.time()
            yield f"data: [STEP 3] hits={len(hits)} (t={t3 - t2:.2f}s)\n\n"

            ok, _ = rag_gate_decision(question, hits, kw_list, need_rag)

            if not ok:
                yield "data: [STEP 3b] ê²€ìƒ‰ ìŠ¤ì½”ì–´ ë‚®ìŒ â†’ Chatìœ¼ë¡œ ì „í™˜\n\n"
                prompt = build_chat_prompt(question)
                mode = "chat"
            else:
                mode = "rag"
                yield "data: [STEP 4] ë¬¸ë§¥ êµ¬ì„± ì‹œì‘\n\n"
                boost_map = keyword_boost(hits, kw_list)
                reranked = rrf_rerank(hits, boost_map)
                final_hits = dedup_by_doc(reranked)
                ctx, refs = build_context_and_refs(final_hits)
                ctx = trim_context_to_budget(ctx)
                prompt = build_rag_prompt(ctx, question, refs)

            yield f"data: [STEP 4] ëª¨ë“œ={mode}\n\n"

            # STEP 5: LLM ìŠ¤íŠ¸ë¦¬ë°
            yield "data: [STEP 5] LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘\n\n"
            for chunk in triton_infer(prompt, stream=True):
                if chunk:
                    yield f"data: {chunk}\n\n"

            yield "data: [END]\n\n"

        except Exception as e:
            err = f"{type(e).__name__}: {e}"
            traceback.print_exc()
            yield f"data: âš ï¸ ì˜¤ë¥˜: {err}\n\n"
            yield "data: [END]\n\n"

    return StreamingResponse(
        event_gen(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
            "Transfer-Encoding": "chunked",
        }
    )
# ---------------------------
# ë¡œì»¬ ì‹¤í–‰
# ---------------------------
if __name__ == "__main__":
    import uvicorn
    # ìš´ì˜ì—ì„œëŠ” --workers 1 ê¶Œì¥ (ì „ì—­ ì»¤ë„¥ì…˜ ì¬ì‚¬ìš© ë° ë””ë²„ê¹… í¸ì˜)
    uvicorn.run(app, host="0.0.0.0", port=8082, reload=False)
