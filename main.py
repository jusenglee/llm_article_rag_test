# -*- coding: utf-8 -*-
import logging, nest_asyncio, time, traceback
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from starlette.responses import StreamingResponse

from query_pipeline import (
    build_rag_objects,
    triton_infer,
    expand_query_kor,
    dense_retrieve_hybrid,
    rrf_rerank,
    build_context,
    decide_rag_needed,
    tokenizer,
)

# ---------------------------
# ì´ˆê¸° ì„¤ì •
# ---------------------------
nest_asyncio.apply()

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger("uvicorn")

app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# ì „ì—­ ê°ì²´ (ë©€í‹°ì›Œì»¤ì¼ ë• í”„ë¡œì„¸ìŠ¤ë³„ ì´ˆê¸°í™”)
qdr = emb = retriever = None


@app.on_event("startup")
async def init_rag():
    global qdr, emb, retriever
    logger.info("ğŸš€ Initializing RAG components...")
    qdr, emb, retriever = build_rag_objects()
    # Warmup: GPU/í† í¬ë‚˜ì´ì €/JIT lazy init ë¹„ìš© ì œê±°
    try:
        _ = tokenizer.encode("warmup")
        _ = emb.get_text_embedding("warmup")
    except Exception as e:
        logger.info(f"Warmup skipped: {e}")
    logger.info("âœ… RAG pipeline ready.\n")


async def triton_stream_async(prompt: str):
    """
    triton_infer(stream=True) ì œë„ˆë ˆì´í„°ë¥¼ ë¹„ë™ê¸° SSEìš©ìœ¼ë¡œ ê°ì‹¸ëŠ” ë˜í¼
    """
    import asyncio

    loop = asyncio.get_event_loop()
    gen = triton_infer(prompt, stream=True)

    while True:
        chunk = await loop.run_in_executor(None, next, gen, None)
        if chunk is None:
            break
        # query_pipeline ìª½ì—ì„œ ì´ë¯¸ strë¡œ ì¤˜ë„ ë°©ì–´ì ìœ¼ë¡œ ì²˜ë¦¬
        text = chunk if isinstance(chunk, str) else chunk.decode("utf-8", errors="ignore")
        yield text


# ---------------------------
# ë¼ìš°íŠ¸
# ---------------------------
@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})


# ---------------------------
# SSE STREAM
# ---------------------------
@app.get("/query/stream")
async def query_stream(question: str):

    async def event_gen():
        import asyncio

        try:
            yield "data: [STEP 0] ì§ˆë¬¸ ìˆ˜ì‹ \n\n"
            await asyncio.sleep(0)

            # STEP 1: ê²Œì´íŠ¸ (RAG / Chat íŒë‹¨)
            t0 = time.time()
            need_rag = decide_rag_needed(question)
            t1 = time.time()
            yield f"data: [STEP 1] ê²Œì´íŠ¸={need_rag} (t={t1 - t0:.2f}s)\n\n"

            context_str = ""
            refs = []

            # --- RAG or Chat ë¶„ê¸° ---
            if not need_rag:
                # ìˆœìˆ˜ Chat ëª¨ë“œ
                yield "data: [STEP 2] RAG ìŠ¤í‚µ â†’ ì¼ë°˜ ëŒ€í™” ì§„í–‰\n\n"
            else:
                yield "data: [STEP 2] í™•ì¥/ê²€ìƒ‰ ì‹œì‘ (RAG)\n\n"

                # 2-1. ì§ˆì˜ í™•ì¥
                expanded_text, kw_list = expand_query_kor(question)
                yield f"data: [STEP 2] í™•ì¥ í‚¤ì›Œë“œ={kw_list}\n\n"

                # 2-2. ë²¡í„° ê²€ìƒ‰
                t2 = time.time()
                hits = dense_retrieve_hybrid(qdr, emb, expanded_text, kw_list)
                t3 = time.time()
                yield f"data: [STEP 3] hits={len(hits)} (t={t3 - t2:.2f}s)\n\n"

                if not hits:
                    yield "data: [STEP 3b] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â†’ Chatìœ¼ë¡œ ì „í™˜\n\n"
                else:
                    # 2-3. ì¬ë­í‚¹ + ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                    yield "data: [STEP 4] ë¬¸ë§¥ êµ¬ì„± ì‹œì‘\n\n"
                    reranked = rrf_rerank(hits, kw_list)
                    context_str, refs = build_context(reranked)

            # 3. í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
            # --- RAG ëª¨ë“œ í”„ë¡¬í”„íŠ¸ ---
            ref_lines = ""
            if context_str:
                if refs:
                    ref_lines = "\n".join(refs)
                else:
                    ref_lines = "(ì¶œì²˜ ì •ë³´ ì—†ìŒ)"

                sys_msg = (
                    "ë‹¹ì‹ ì€ ì‚¬ìš©ìë¥¼ ë³´ì¡°í•˜ëŠ” LLMì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ê·¼ê±°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. "
                    "ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ 'ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤'ë¼ê³ ë§Œ ë§í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”. "
                    "ê°€ëŠ¥í•˜ë©´ ë¬¸ì¥ ë‚´ì— ê·¼ê±° ë²ˆí˜¸ ê°ì£¼ë¥¼ í‘œì‹œí•˜ì„¸ìš”."
                )
                user_msg = f"""
                ë‹¤ìŒì€ ê´€ë ¨ ë¬¸ì„œ ë°œì·Œì…ë‹ˆë‹¤(ë²ˆí˜¸=ì¶œì²˜):
                {context_str}
                
                (ì¶œì²˜ ë²ˆí˜¸ ë§¤í•‘)
                {ref_lines}
                
                ì§ˆë¬¸: {question}
                
                ìš”êµ¬ì‚¬í•­:
                - ë¬¸ì¥ ë‚´ì— [1], [2] í˜•íƒœë¡œ ê·¼ê±° ë²ˆí˜¸ë¥¼ ë‹¬ì•„ì£¼ì„¸ìš”.
                - ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì“°ì§€ ë§ˆì„¸ìš”(ì¶”ê°€ ì§€ì‹ ê¸ˆì§€).
                - ë§ˆì§€ë§‰ ì¤„ì— 'ì°¸ê³ ë¬¸í—Œ:' ë’¤ì— ë…¼ë¬¸ ì œëª©ì„ í•¨ê»˜ ë‚˜ì—´í•˜ì„¸ìš”. ì˜ˆì‹œ: ì°¸ê³ ë¬¸í—Œ: [1] ì œëª©A, [2] ì œëª©B
                """

            # --- ì¼ë°˜ Chat í”„ë¡¬í”„íŠ¸ ---
            else:
                sys_msg = (
                    "ë„ˆëŠ” ì¹œì ˆí•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. "
                    "ì§ˆë¬¸ì´ íŠ¹ë³„íˆ ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ìš”êµ¬í•˜ì§€ ì•ŠëŠ” í•œ, ê¸°ë³¸ì ìœ¼ë¡œ **í•œêµ­ì–´**ë¡œ ë‹µë³€í•´ë¼."
                )
                user_msg = question

            # tokenizerê°€ ìˆëŠ” ê²½ìš° chat template í™œìš©
            try:
                messages = [
                    {"role": "system", "content": sys_msg},
                    {"role": "user", "content": user_msg},
                ]
                prompt = tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=True
                )
            except Exception:
                # í…œí”Œë¦¿ ë¯¸ì§€ì› ëª¨ë¸ì¼ ë•Œ fallback
                prompt = f"<|system|>\n{sys_msg}\n<|user|>\n{user_msg}\n<|assistant|>\n"

            # -------------------------------------------------
            # STEP 5 (ê³µí†µ ìŠ¤íŠ¸ë¦¬ë°)
            # -------------------------------------------------
            yield "data: [STEP 5] LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘\n\n"

            async for chunk in triton_stream_async(prompt):
                # ì—¬ê¸°ì„œëŠ” chunkê°€ ì´ë¯¸ UTF-8 safe str ì´ë¼ê³  ê°€ì •
                text = chunk if isinstance(chunk, str) else chunk.decode("utf-8", errors="ignore")
                if text.strip():
                    # SSE í˜•ì‹
                    yield f"data: {text}\n\n"

            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
            yield "data: [END]\n\n"

        except Exception as e:
            err = f"{type(e).__name__}: {e}"
            traceback.print_exc()
            yield f"data: âš ï¸ ì˜¤ë¥˜: {err}\n\n"
            yield "data: [END]\n\n"

    return StreamingResponse(
        event_gen(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
            "Transfer-Encoding": "chunked",
        },
    )


# ---------------------------
# ë¡œì»¬ ì‹¤í–‰
# ---------------------------
if __name__ == "__main__":
    import uvicorn

    # ìš´ì˜ì—ì„œëŠ” --workers 1 ê¶Œì¥ (ì „ì—­ ì»¤ë„¥ì…˜ ì¬ì‚¬ìš© ë° ë””ë²„ê¹… í¸ì˜)
    uvicorn.run(app, host="0.0.0.0", port=8082, reload=False)
