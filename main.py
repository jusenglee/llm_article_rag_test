# -*- coding: utf-8 -*-
import logging, nest_asyncio, time, traceback
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from starlette.responses import StreamingResponse

from rag_pipeline import decide_rag_needed, run_rag_ab_compare
from rag_store import build_rag_objects_dual
from retrieval import expand_query_kor, dense_retrieve_hybrid, rrf_rerank, build_context
from settings import COLLECTION, COLLECTION_B, TRITON_URL
from triton_client import triton_infer, get_tokenizer_for_model, ensure_single_model_loaded, unload_model_safe, \
    get_triton_client

# ---------------------------
# ì´ˆê¸° ì„¤ì •
# ---------------------------
nest_asyncio.apply()

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger("uvicorn")

app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# ì „ì—­ ê°ì²´ (ë©€í‹°ì›Œì»¤ì¼ ë• í”„ë¡œì„¸ìŠ¤ë³„ ì´ˆê¸°í™”)
qdr_a = emb_a = retriever_a = None
qdr_b = emb_b = retriever_b = None

# í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì˜¤ëŠ” model í‚¤ â†’ Triton ëª¨ë¸ ì´ë¦„ ë§¤í•‘
MODEL_MAP = {
    "gpt": "gpt_oss_0",
    "gemma": "gemma_vllm_0",
    # EXAONE ë¶™ì´ë©´ ì—¬ê¸°
     "exaone": "EXAONE_0",
}


@app.on_event("startup")
async def init_rag():
    global qdr_a, emb_a, retriever_a, qdr_b, emb_b, retriever_b
    logger.info("ğŸš€ Initializing RAG components (dual)...")
    qdr_a, emb_a, retriever_a, qdr_b, emb_b, retriever_b = build_rag_objects_dual()
    logger.info("âœ… RAG pipeline (A/B) ready.\n")


async def triton_stream_async(model_name: str, prompt: str):
    """
    triton_infer(stream=True) ì œë„ˆë ˆì´í„°ë¥¼ ë¹„ë™ê¸° SSEìš©ìœ¼ë¡œ ê°ì‹¸ëŠ” ë˜í¼
    - ë°˜ë“œì‹œ model_nameì„ ì¸ìë¡œ ë°›ì•„ì„œ, ì–´ë–¤ ëª¨ë¸ì„ ì“¸ì§€ FastAPIì—ì„œ ê²°ì •
    """
    import asyncio

    loop = asyncio.get_event_loop()
    gen = triton_infer(model_name, prompt, stream=True)

    i = 0
    while True:
        chunk = await loop.run_in_executor(None, next, gen, None)
        if chunk is None:
            logger.info("[DEBUG] RAW_CHUNK EOF")
            break

        text = chunk if isinstance(chunk, str) else chunk.decode("utf-8", errors="ignore")

        i += 1

        yield text


# ---------------------------
# ë¼ìš°íŠ¸
# ---------------------------
@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})


# ---------------------------
# SSE STREAM
# ---------------------------
@app.get("/query/stream")
async def query_stream(question: str, model: str = "gpt"):
    """
    í´ë¼ì´ì–¸íŠ¸ì—ì„œ:
      /query/stream?model=gpt&question=...
    ì´ëŸ° ì‹ìœ¼ë¡œ í˜¸ì¶œ (HTMLì—ì„œ select ë°•ìŠ¤ë¡œ model ê°’ì„ ë„˜ê¹€)

    1) model í‚¤ â†’ Triton ëª¨ë¸ ì´ë¦„ ë³€í™˜
    2) ensure_single_model_loaded(model_name) í˜¸ì¶œ
    3) RAG / Chat í”„ë¡¬í”„íŠ¸ ìƒì„± + triton_stream_async(model_name, ...)
    4) ëë‚˜ë©´ finallyì—ì„œ unload_model_safe(model_name)
    """

    model_key = model.lower()
    if model_key not in MODEL_MAP:
        raise HTTPException(status_code=400, detail=f"Unknown model: {model}")

    model_name = MODEL_MAP[model_key]
    logger.info(f"[QUERY] model_key={model_key}, model_name={model_name}, question={question!r}")
    tok = get_tokenizer_for_model(model_name)

    async def event_gen():
        import asyncio

        await asyncio.sleep(0)  # ì´ë²¤íŠ¸ ë£¨í”„ ì–‘ë³´ìš©

        # 0. Tritonì—ì„œ ëª¨ë¸ ë¡œë“œ ê³¼ì •ì„ ì‚¬ìš©ìì—ê²Œ ê·¸ëŒ€ë¡œ ë…¸ì¶œ
        try:
            cli = get_triton_client()

            # 0-1) ë ˆí¬ì§€í† ë¦¬ ì¸ë±ìŠ¤ ì¡°íšŒ
            yield f"data: [MODEL] Triton ì—°ê²° ({TRITON_URL}) í›„ ëª¨ë¸ ë¡œë“œ ì‹œë„ ì¤‘...\n\n"
            repo = cli.get_model_repository_index()
            names = [getattr(m, "name", "?") for m in getattr(repo, "models", [])]
            yield f"data: [MODEL] í˜„ì¬ ë“±ë¡ëœ ëª¨ë¸: {', '.join(names)}\n\n"

            # 0-2) target ì´ì™¸ ëª¨ë¸ UNLOAD
            for m in getattr(repo, "models", []):
                name = getattr(m, "name", None)
                if not name or name == model_name:
                    continue
                try:
                    if cli.is_model_ready(name):
                        yield f"data: [MODEL] ë‹¤ë¥¸ ëª¨ë¸ ì–¸ë¡œë“œ ìš”ì²­: {name}\n\n"
                        cli.unload_model(name)
                        yield f"data: [MODEL] ì–¸ë¡œë“œ ì™„ë£Œ: {name}\n\n"
                except Exception as e:
                    logger.warning(f"[TRITON] unload_model({name}) failed: {e}")
                    yield f"data: [MODEL] ì–¸ë¡œë“œ ì‹¤íŒ¨({name}): {type(e).__name__}: {e}\n\n"

            # 0-3) target ëª¨ë¸ ìƒíƒœ í™•ì¸
            try:
                if cli.is_model_ready(model_name):
                    yield f"data: [MODEL] {model_name} ì´ë¯¸ READY ìƒíƒœì…ë‹ˆë‹¤.\n\n"
                else:
                    # 0-4) target ëª¨ë¸ ë¡œë“œ ì‹œì‘
                    yield f"data: [MODEL] {model_name} ë¡œë“œ ì‹œì‘...\n\n"
                    cli.load_model(model_name)
                    start = time.time()
                    timeout = 120.0

                    # 0-5) READY ë  ë•Œê¹Œì§€ polling + ì§„í–‰ ìƒí™© SSE ì „ì†¡
                    while True:
                        await asyncio.sleep(0.5)
                        elapsed = time.time() - start

                        try:
                            if cli.is_model_ready(model_name):
                                yield f"data: [MODEL] {model_name} READY (t={elapsed:.2f}s)\n\n"
                                break
                        except Exception as e:
                            logger.warning(f"[TRITON] is_model_ready({model_name}) check failed: {e}")
                            yield f"data: [MODEL] ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {type(e).__name__}: {e}\n\n"

                        if elapsed > timeout:
                            raise TimeoutError(
                                f"Timeout while waiting for model {model_name} to be READY"
                            )

                        # ì§„í–‰ ì¤‘ì¸ ìƒíƒœë„ ê³„ì† ì´ì¤Œ
                        yield f"data: [MODEL] {model_name} ë¡œë”© ì¤‘... (elapsed={elapsed:.1f}s)\n\n"

            except Exception as e:
                raise e

        except Exception as e:
            err = f"Triton ëª¨ë¸ ë¡œë“œ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜: {type(e).__name__}: {e}"
            traceback.print_exc()
            yield f"data: âš ï¸ {err}\n\n"
            yield "data: [END]\n\n"
            return

        try:
            yield "data: [STEP 0] ì§ˆë¬¸ ìˆ˜ì‹ \n\n"

            # 1: ê²Œì´íŠ¸ (RAG / Chat íŒë‹¨)
            t0 = time.time()
            need_rag = decide_rag_needed(question, model_name=model_name)
            t1 = time.time()
            yield f"data: [STEP 1] ê²Œì´íŠ¸={need_rag} (t={t1 - t0:.2f}s)\n\n"

            # ì§ˆì˜ í™•ì¥ì€ í•œ ë²ˆë§Œ
            expanded_text = None
            kw_list = None

            # RAG ê²°ê³¼ ì»¨í…ìŠ¤íŠ¸
            context_a = ""
            refs_a = []
            context_b = ""
            refs_b = []

            # RAG or Chat ë¶„ê¸°
            if not need_rag:
                # ìˆœìˆ˜ Chat ëª¨ë“œ
                yield "data: [STEP 2] RAG ìŠ¤í‚µ â†’ ì¼ë°˜ ëŒ€í™” ì§„í–‰\n\n"
            else:
                yield "data: [STEP 2] í™•ì¥/ê²€ìƒ‰ ì‹œì‘ (RAG, A/B ë¹„êµ)\n\n"

                # ì—¬ê¸°ì„œ ì „ì²´ RAG A/B ë¹„êµ í•œ ë²ˆì— ìˆ˜í–‰
                res_map = run_rag_ab_compare(
                    query=question,
                    with_llm=False,          # ì—¬ê¸°ì„œëŠ” ì»¨í…ìŠ¤íŠ¸ê¹Œì§€ë§Œ, LLMì€ ì•„ë˜ì—ì„œ ìŠ¤íŠ¸ë¦¬ë°
                    model_name=model_name,
                )
                res_a = res_map["A"]
                res_b = res_map["B"]

                # í™•ì¥ ì¿¼ë¦¬ / í‚¤ì›Œë“œ ë¡œê·¸
                yield f"data: [EXPAND] í™•ì¥ ì¿¼ë¦¬(Aê¸°ì¤€) = {res_a.expanded_query}\n\n"
                yield f"data: [EXPAND] í‚¤ì›Œë“œ(Aê¸°ì¤€) = {res_a.keywords}\n\n"

                # ì„±ëŠ¥ íƒ€ì´ë°ì„ SSEë¡œ ì „ì†¡
                ta = res_a.timings
                tb = res_b.timings

                yield (
                    "data: [PERF-A] "
                    f"í™•ì¥(expand)={ta.get('expand_query', 0.0):.3f}s, "
                    f"ê²€ìƒ‰(dense_total)={ta.get('dense_search', 0.0):.3f}s, "
                    f"ë¦¬ë­í¬(rerank)={ta.get('rerank', 0.0):.3f}s, "
                    f"ì»¨í…ìŠ¤íŠ¸(ctx)={ta.get('build_context', 0.0):.3f}s\n\n"
                )
                yield (
                    "data: [PERF-A] "
                    f"í™•ì¥(expand)={tb.get('expand_query', 0.0):.3f}s, "
                    f"ê²€ìƒ‰(dense_total)={tb.get('dense_search', 0.0):.3f}s, "
                    f"ë¦¬ë­í¬(rerank)={tb.get('rerank', 0.0):.3f}s, "
                    f"ì»¨í…ìŠ¤íŠ¸(ctx)={tb.get('build_context', 0.0):.3f}s\n\n"
                )

                # ìƒìœ„ ë¬¸ì„œ ëª©ë¡ë„ SSEë¡œ ì „ì†¡
                yield "data: [HITS-A] ----- A ìŠ¤íƒ ìƒìœ„ ë¬¸ì„œ ëª©ë¡ -----\n\n"
                for i, h in enumerate(res_a.reranked_hits[:5], start=1):
                    raw = (h.payload or {}).get("_node_text") or ""
                    title = " ".join(str(raw).splitlines())
                    yield f"data: [HITS-A] [{i}] {title}\n\n"

                yield "data: [HITS-B] ----- B ìŠ¤íƒ ìƒìœ„ ë¬¸ì„œ ëª©ë¡ -----\n\n"
                for i, h in enumerate(res_b.reranked_hits[:5], start=1):
                    raw = (h.payload or {}).get("_node_text") or ""
                    title = " ".join(str(raw).splitlines())
                    yield f"data: [HITS-B] [{i}] {title}\n\n"

                context_a, refs_a = res_a.context, res_a.refs
                context_b, refs_b = res_b.context, res_b.refs
            # -------- í”„ë¡¬í”„íŠ¸ ë¹Œë“œ & ìŠ¤íŠ¸ë¦¬ë° --------

            # RAGê°€ í•„ìš” ì—†ê±°ë‚˜, ë‘˜ ë‹¤ ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì—ˆìœ¼ë©´: ë‹¨ì¼ Chat ëª¨ë“œ
            if not need_rag or (not context_a and not context_b):
                sys_msg = (
                    "ë„ˆëŠ” ì¹œì ˆí•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. "
                    "ì§ˆë¬¸ì´ íŠ¹ë³„íˆ ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ìš”êµ¬í•˜ì§€ ì•ŠëŠ” í•œ, ê¸°ë³¸ì ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ë¼."
                )
                user_msg = question

                try:
                    messages = [
                        {"role": "system", "content": sys_msg},
                        {"role": "user", "content": user_msg},
                    ]
                    prompt = tok.apply_chat_template(
                        messages, tokenize=False, add_generation_prompt=True
                    )
                except Exception:
                    prompt = f"<|system|>\n{sys_msg}\n<|user|>\n{user_msg}\n<|assistant|>\n"

                yield f"data: [STEP 5] LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (ì¼ë°˜ Chat, model={model_name})\n\n"

                async for chunk in triton_stream_async(model_name, prompt):
                    text = chunk if isinstance(chunk, str) else chunk.decode("utf-8", errors="ignore")
                    if text.strip():
                        yield f"data: {text}\n\n"

                yield "data: [END]\n\n"
                return

            # RAG A/B ë¹„êµ
            # ---------- A ìŠ¤íƒ ì‘ë‹µ ----------
            if context_a:
                ref_lines_a = "\n".join(refs_a) if refs_a else "(ì¶œì²˜ ì •ë³´ ì—†ìŒ)"

                sys_msg_a = (
                    "ë‹¹ì‹ ì€ ê³¼í•™Â·ê¸°ìˆ  ë…¼ë¬¸ì„ ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ëŠ” ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
                    "- ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸(ë¬¸ì„œ ë°œì·Œ)ì—ì„œë§Œ ê·¼ê±°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n"
                    "- ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ 'ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'ë¼ê³ ë§Œ ë§í•˜ê³ , ì„ì˜ë¡œ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.\n"
                    "- í•œêµ­ì–´ ë¬¸ì¥ì—ì„œ ì •ìƒì ì¸ ë„ì–´ì“°ê¸°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n"
                    "- ê·¸ ì•ˆìª½ì—ë§Œ ì‹¤ì œ í•œêµ­ì–´ ë‹µë³€ì„ ì‘ì„±í•˜ê³ , ê·¸ ë°–ì—ëŠ” ì–´ë–¤ ë¶„ì„/ì„¤ëª…/ê³„íš ë¬¸ì¥ë„ ì“°ì§€ ë§ˆì„¸ìš”.\n"
                    "ì´ ì‘ë‹µì€ [A ìŠ¤íƒ] ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤."
                )
                user_msg_a = f"""
ë‹¤ìŒì€ [A ìŠ¤íƒ]ì—ì„œ ê²€ìƒ‰í•œ ê´€ë ¨ ë¬¸ì„œ ë°œì·Œì…ë‹ˆë‹¤. ê° ë¬¸ë‹¨ ì•ì˜ ë²ˆí˜¸ëŠ” ì¶œì²˜ ë²ˆí˜¸ì…ë‹ˆë‹¤.

[ì»¨í…ìŠ¤íŠ¸ ë°œì·Œ ì‹œì‘]
{context_a}
[ì»¨í…ìŠ¤íŠ¸ ë°œì·Œ ë]

(ì¶œì²˜ ë²ˆí˜¸ ë§¤í•‘)
{ref_lines_a}

ì‚¬ìš©ìì˜ ì§ˆë¬¸:
{question}

ë‹µë³€ í˜•ì‹ ê°€ì´ë“œë¼ì¸(ì•„ì£¼ ì¤‘ìš”):
1. ì²« ë¬¸ë‹¨ì— 2~3ë¬¸ì¥ìœ¼ë¡œ ì „ì²´ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
2. ê·¸ ë‹¤ìŒì—ëŠ” "1. ì†Œì œëª©" í˜•ì‹ì˜ ë²ˆí˜¸ ë§¤ê¸°ê¸° ëª©ë¡ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í•©ë‹ˆë‹¤. ì†Œì œëª©ì€ ë‚´ìš©ì„ ì••ì¶•í•˜ì—¬ ì„ì˜ë¡œ ì‘ì„±í•˜ì„¸ìš”
   - ê° í•­ëª©ì€ "1. ì†Œì œëª© [1][3]" ì²˜ëŸ¼ ê´€ë ¨ ì¶œì²˜ ë²ˆí˜¸ë¥¼ ëŒ€ê´„í˜¸ë¡œ í‘œê¸°í•©ë‹ˆë‹¤.
   - ì†Œì œëª© ì•„ë˜ ì¤„ì—ì„œ 2~4ë¬¸ì¥ ì •ë„ë¡œ ì„¤ëª…ì„ ë§ë¶™ì…ë‹ˆë‹¤.
3. ë¬¸ì¥ ì¤‘ê°„ì— ê·¼ê±°ë¥¼ ë‹¬ ë•ŒëŠ” "â€¦ë¼ëŠ” ì ì´ ë³´ê³ ë˜ì—ˆìŠµë‹ˆë‹¤[1][3]."ì²˜ëŸ¼ [1] í˜•íƒœì˜ ì¸ìš© ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
4. í•œêµ­ì–´ ë¬¸ì¥ ì‚¬ì´ì—ëŠ” ì¼ë°˜ì ì¸ ë„ì–´ì“°ê¸°ë¥¼ ìœ ì§€í•˜ê³ ,
   'ì˜í•™ê¸°ìˆ ì˜ìµœì‹ ë™í–¥ì€'ì²˜ëŸ¼ ë‹¨ì–´ë¥¼ ëª¨ë‘ ë¶™ì—¬ ì“°ì§€ ë§ê³ 
   'ì˜í•™ ê¸°ìˆ ì˜ ìµœì‹  ë™í–¥ì€'ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ ë„ì–´ì“°ê¸°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
5. ë§ˆì§€ë§‰ì—ëŠ” ì•„ë˜ ì˜ˆì‹œì²˜ëŸ¼ ì°¸ê³ ë¬¸í—Œ ì„¹ì…˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

ì°¸ê³ ë¬¸í—Œ:
[1] ë…¼ë¬¸ ì œëª©A
[2] ë…¼ë¬¸ ì œëª©B
[3] ë…¼ë¬¸ ì œëª©C
"""

                try:
                    messages_a = [
                        {"role": "system", "content": sys_msg_a},
                        {"role": "user", "content": user_msg_a},
                    ]
                    prompt_a = tok.apply_chat_template(
                        messages_a, tokenize=False, add_generation_prompt=True
                    )
                except Exception:
                    prompt_a = f"<|system|>\n{sys_msg_a}\n<|user|>\n{user_msg_a}\n<|assistant|>\n"

                yield "data: \n\n"
                yield "data: =============================\n\n"
                yield "data: [RAG-A] ì„ë² ë”©/ë²¡í„°DB ìŠ¤íƒ A ì‘ë‹µ\n\n"
                yield "data: =============================\n\n"

                async for chunk in triton_stream_async(model_name, prompt_a):
                    text = chunk if isinstance(chunk, str) else chunk.decode("utf-8", errors="ignore")
                    if text.strip():
                        yield f"data: [A] {text}\n\n"

            # ---------- B ìŠ¤íƒ ì‘ë‹µ ----------
            if context_b:
                ref_lines_b = "\n".join(refs_b) if refs_b else "(ì¶œì²˜ ì •ë³´ ì—†ìŒ)"

                sys_msg_b = (
                    "ë‹¹ì‹ ì€ ê³¼í•™Â·ê¸°ìˆ  ë…¼ë¬¸ì„ ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ëŠ” ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
                    "- ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸(ë¬¸ì„œ ë°œì·Œ)ì—ì„œë§Œ ê·¼ê±°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n"
                    "- ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ 'ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.'ë¼ê³ ë§Œ ë§í•˜ê³ , ì„ì˜ë¡œ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.\n"
                    "- í•œêµ­ì–´ ë¬¸ì¥ì—ì„œ ì •ìƒì ì¸ ë„ì–´ì“°ê¸°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n"
                    "- ê·¸ ì•ˆìª½ì—ë§Œ ì‹¤ì œ í•œêµ­ì–´ ë‹µë³€ì„ ì‘ì„±í•˜ê³ , ê·¸ ë°–ì—ëŠ” ì–´ë–¤ ë¶„ì„/ì„¤ëª…/ê³„íš ë¬¸ì¥ë„ ì“°ì§€ ë§ˆì„¸ìš”.\n"
                    "ì´ ì‘ë‹µì€ [B ìŠ¤íƒ] ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤."
                )
                user_msg_b = f"""
ë‹¤ìŒì€ [B ìŠ¤íƒ]ì—ì„œ ê²€ìƒ‰í•œ ê´€ë ¨ ë¬¸ì„œ ë°œì·Œì…ë‹ˆë‹¤. ê° ë¬¸ë‹¨ ì•ì˜ ë²ˆí˜¸ëŠ” ì¶œì²˜ ë²ˆí˜¸ì…ë‹ˆë‹¤.

[ì»¨í…ìŠ¤íŠ¸ ë°œì·Œ ì‹œì‘]
{context_b}
[ì»¨í…ìŠ¤íŠ¸ ë°œì·Œ ë]

(ì¶œì²˜ ë²ˆí˜¸ ë§¤í•‘)
{ref_lines_b}

ì‚¬ìš©ìì˜ ì§ˆë¬¸:
{question}

ë‹µë³€ í˜•ì‹ ê°€ì´ë“œë¼ì¸(ì•„ì£¼ ì¤‘ìš”):
1. ì²« ë¬¸ë‹¨ì— 2~3ë¬¸ì¥ìœ¼ë¡œ ì „ì²´ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
2. ê·¸ ë‹¤ìŒì—ëŠ” "1. ì†Œì œëª©" í˜•ì‹ì˜ ë²ˆí˜¸ ë§¤ê¸°ê¸° ëª©ë¡ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í•©ë‹ˆë‹¤. ì†Œì œëª©ì€ ë‚´ìš©ì„ ì••ì¶•í•˜ì—¬ ì„ì˜ë¡œ ì‘ì„±í•˜ì„¸ìš”
   - ê° í•­ëª©ì€ "1. ì†Œì œëª© [1][3]" ì²˜ëŸ¼ ê´€ë ¨ ì¶œì²˜ ë²ˆí˜¸ë¥¼ ëŒ€ê´„í˜¸ë¡œ í‘œê¸°í•©ë‹ˆë‹¤.
   - ì†Œì œëª© ì•„ë˜ ì¤„ì—ì„œ 2~4ë¬¸ì¥ ì •ë„ë¡œ ì„¤ëª…ì„ ë§ë¶™ì…ë‹ˆë‹¤.
3. ë¬¸ì¥ ì¤‘ê°„ì— ê·¼ê±°ë¥¼ ë‹¬ ë•ŒëŠ” "â€¦ë¼ëŠ” ì ì´ ë³´ê³ ë˜ì—ˆìŠµë‹ˆë‹¤[1][3]."ì²˜ëŸ¼ [1] í˜•íƒœì˜ ì¸ìš© ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
4. í•œêµ­ì–´ ë¬¸ì¥ ì‚¬ì´ì—ëŠ” ì¼ë°˜ì ì¸ ë„ì–´ì“°ê¸°ë¥¼ ìœ ì§€í•˜ê³ ,
   'ì˜í•™ê¸°ìˆ ì˜ìµœì‹ ë™í–¥ì€'ì²˜ëŸ¼ ë‹¨ì–´ë¥¼ ëª¨ë‘ ë¶™ì—¬ ì“°ì§€ ë§ê³ 
   'ì˜í•™ ê¸°ìˆ ì˜ ìµœì‹  ë™í–¥ì€'ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ ë„ì–´ì“°ê¸°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
5. ë§ˆì§€ë§‰ì—ëŠ” ì•„ë˜ ì˜ˆì‹œì²˜ëŸ¼ ì°¸ê³ ë¬¸í—Œ ì„¹ì…˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

ì°¸ê³ ë¬¸í—Œ:
[1] ë…¼ë¬¸ ì œëª©A
[2] ë…¼ë¬¸ ì œëª©B
[3] ë…¼ë¬¸ ì œëª©C

"""

                try:
                    messages_b = [
                        {"role": "system", "content": sys_msg_b},
                        {"role": "user", "content": user_msg_b},
                    ]
                    prompt_b = tok.apply_chat_template(
                        messages_b, tokenize=False, add_generation_prompt=True
                    )
                except Exception:
                    prompt_b = f"<|system|>\n{sys_msg_b}\n<|user|>\n{user_msg_b}\n<|assistant|>\n"

                yield "data: \n\n"
                yield "data: =============================\n\n"
                yield "data: [RAG-B] ì„ë² ë”©/ë²¡í„°DB ìŠ¤íƒ B ì‘ë‹µ\n\n"
                yield "data: =============================\n\n"

                async for chunk in triton_stream_async(model_name, prompt_b):
                    text = chunk if isinstance(chunk, str) else chunk.decode("utf-8", errors="ignore")
                    if text.strip():
                        yield f"data: [B] {text}\n\n"

            # ìµœì¢… ì¢…ë£Œ
            yield "data: [END]\n\n"

        except Exception as e:
            err = f"{type(e).__name__}: {e}"
            traceback.print_exc()
            yield f"data: âš ï¸ ì˜¤ë¥˜: {err}\n\n"
            yield "data: [END]\n\n"

        finally:
            # ì´ ìš”ì²­ì—ì„œ ì‚¬ìš©í•œ ëª¨ë¸ì€ ë¬´ì¡°ê±´ ë‚´ë ¤ì¤€ë‹¤ (í•œ ë²ˆì— í•˜ë‚˜ ì „ëµ)
            try:
                logger.info(f"[TRITON] unload_model_safe({model_name})")
                unload_model_safe(model_name)
            except Exception as e:
                logger.warning(f"[TRITON] unload_model_safe({model_name}) failed: {e}")

    return StreamingResponse(
        event_gen(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
            "Transfer-Encoding": "chunked",
        },
    )


# ---------------------------
# ë¡œì»¬ ì‹¤í–‰
# ---------------------------
if __name__ == "__main__":
    import uvicorn

    # ìš´ì˜ì—ì„œëŠ” --workers 1 ê¶Œì¥ (ì „ì—­ ì»¤ë„¥ì…˜ ì¬ì‚¬ìš© ë° ë””ë²„ê¹… í¸ì˜)
    uvicorn.run(app, host="0.0.0.0", port=8082, reload=False)
