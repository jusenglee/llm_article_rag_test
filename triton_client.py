# rag_pipeline/triton_client.py
import json
import threading
import time
import numpy as np
from typing import Dict, List
from transformers import AutoTokenizer
from tritonclient.grpc import InferenceServerClient, InferInput, InferRequestedOutput
from settings import (
    TRITON_URL, TOKENIZER_MAP, MAX_TOKENS, TEMPERATURE, TOP_P,
)
from settings import logger  # ê³µìš© logger

_triton_client: InferenceServerClient | None = None
_tokenizers: Dict[str, AutoTokenizer] = {}

ASSISTANT_FINAL_MARKER = "assistantfinal"


def _is_gpt_oss_model(model_name: str) -> bool:
    """
    gpt-oss ê³„ì—´ ëª¨ë¸ íŒë³„ìš© í—¬í¼.
    - ì´ë¦„ ê·œì¹™ì€ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ í•„ìš”ì‹œ ìˆ˜ì •.
    """
    name = model_name.lower()
    return ("gpt" in name) and ("oss" in name)

def get_tokenizer_for_model(model_name: str) -> AutoTokenizer:
    if model_name not in _tokenizers:
        tok_id = TOKENIZER_MAP[model_name]
        _tokenizers[model_name] = AutoTokenizer.from_pretrained(
            tok_id, trust_remote_code=True
        )
    return _tokenizers[model_name]


def _get_prompt_tokens(model_name: str, prompt: str) -> int:
    """
    ì£¼ì–´ì§„ ëª¨ë¸ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ í† í° ê¸¸ì´ ê³„ì‚°.
    í† í¬ë‚˜ì´ì € ë¬¸ì œ ë°œìƒ ì‹œì—ëŠ” len(prompt)ë¡œ ì•„ì£¼ ëŸ¬í”„í•˜ê²Œ fallback.
    """
    try:
        tok = get_tokenizer_for_model(model_name)
        # special tokenì€ ì´ë¯¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë“¤ì–´ê°€ ìˆì„ ê°€ëŠ¥ì„± ìˆìœ¼ë‹ˆ False
        ids = tok.encode(prompt, add_special_tokens=False)
        return len(ids)
    except Exception as e:
        logger.warning(f"[TRITON] prompt token ê³„ì‚° ì‹¤íŒ¨, fallback ì‚¬ìš©: {e}")
        # ì™„ì „ ë¹„ì—ˆìœ¼ë©´ 0, ì•„ë‹ˆë©´ ê¸€ì ìˆ˜ ê¸°ì¤€ ëŸ¬í”„ ì¶”ì •
        return max(1, len(prompt) // 2)


def _compute_max_new_tokens(
        model_name: str,
        prompt: str,
        max_tokens_hint: int | None = None,
) -> int:
    """
    - í† í¬ë‚˜ì´ì €ì˜ model_max_length(ì—†ìœ¼ë©´ 8192 ì¶”ì •)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ
      prompt_tokens + max_new_tokens <= max_seq_len - margin ì„ ë§Œì¡±í•˜ë„ë¡ ì¡°ì •.
    - max_tokens_hint(= ì¸ìë¡œ ë°›ì€ max_tokens)ëŠ” ìƒí•œ(cap)ìœ¼ë¡œë§Œ ì‚¬ìš©.
    """
    prompt_tokens = _get_prompt_tokens(model_name, prompt)

    try:
        tok = get_tokenizer_for_model(model_name)
        max_seq_len = getattr(tok, "model_max_length", 8192)
        # HF ìª½ì—ì„œ ì¢…ì¢… ì—„ì²­ í° ê°’(1e30 ê°™ì€) ë„£ì–´ë‘ëŠ” ê²½ìš° ë°©ì–´
        if max_seq_len is None or max_seq_len > 100_000:
            max_seq_len = 8192
    except Exception:
        max_seq_len = 8192

    SAFETY_MARGIN = 256      # ì—¬ìœ  ë²„í¼
    MIN_NEW_TOKENS = 64      # ìµœì†Œ ìƒì„± í† í°

    # settings.MAX_TOKENS ë¥¼ ê¸°ë³¸ ìƒí•œìœ¼ë¡œ, ì¸ìë¡œ ë“¤ì–´ì˜¤ë©´ ê·¸ê²ƒìœ¼ë¡œ override
    cap = int(max_tokens_hint) if max_tokens_hint is not None else int(MAX_TOKENS)

    available = max_seq_len - prompt_tokens - SAFETY_MARGIN
    if available <= 0:
        logger.warning(
            f"[TRITON] promptê°€ ì´ë¯¸ max_seq_lenì„ ê±°ì˜ ë‹¤ ì“´ ìƒíƒœì…ë‹ˆë‹¤: "
            f"prompt_tokens={prompt_tokens}, max_seq_len={max_seq_len}"
        )
        # ê·¸ë˜ë„ ìµœì†Œí•œ ì¡°ê¸ˆì€ ìƒì„±í•˜ë„ë¡
        return max(MIN_NEW_TOKENS, min(cap, 128))

    max_new = min(cap, available)
    return max(MIN_NEW_TOKENS, max_new)


def extract_final_answer(raw: str) -> str:
    """gpt-ossê°€ analysis/assistantfinal í¬ë§·ìœ¼ë¡œ ë±‰ì„ ë•Œ, ìµœì¢… ë‹µë³€ë§Œ ì¶”ì¶œ."""
    if not raw:
        return ""

    text = str(raw).strip()
    idx = text.rfind(ASSISTANT_FINAL_MARKER)
    if idx == -1:
        # ë§ˆì»¤ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì›ë³¸ ë°˜í™˜ (ë‹¤ë¥¸ ëª¨ë¸ ëŒ€ë¹„ ì•ˆì „ì¥ì¹˜)
        return text

    final = text[idx + len(ASSISTANT_FINAL_MARKER):]
    # ì½œë¡ /ê³µë°± ì •ë¦¬
    final = final.lstrip(" :\n\t")
    logger.info(final)
    return final.strip()

def stream_after_assistantfinal(chunks):
    """
    gpt-oss ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼(chunks)ë¥¼ ë°›ì•„ì„œ
    'assistantfinal' ì´í›„ í…ìŠ¤íŠ¸ë§Œ yieldí•˜ëŠ” ì œë„ˆë ˆì´í„°.
    """
    marker = ASSISTANT_FINAL_MARKER.lower()
    seen = False
    buf = ""

    for chunk in chunks:
        if not chunk:
            continue

        buf += chunk

        if not seen:
            pos = buf.lower().find(marker)
            if pos == -1:
                # ì•„ì§ ë§ˆì»¤ ì•ˆ ë‚˜ì™”ìœ¼ë©´ ê³„ì† ë²„í¼ì—ë§Œ ìŒ“ìŒ
                continue

            # ì²˜ìŒìœ¼ë¡œ ë§ˆì»¤ë¥¼ ë°œê²¬í•œ ì‹œì 
            seen = True
            start = pos + len(ASSISTANT_FINAL_MARKER)
            # ë§ˆì»¤ ì•ë¶€ë¶„ì€ ë²„ë¦¬ê³ , ë§ˆì»¤ ë’¤ë¶€í„° ì‚¬ìš©
            buf = buf[start:]
            buf = buf.lstrip(" :\n\t")

            if not buf:
                continue

        # ì—¬ê¸°ë¶€í„°ëŠ” ì „ë¶€ 'ìµœì¢… ë‹µë³€'ì— í•´ë‹¹
        yield buf
        buf = ""

    # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ í›„ ë§ˆë¬´ë¦¬ ì²˜ë¦¬
    if seen and buf:
        # assistantfinal ì´í›„ ë‚¨ì€ ì°Œêº¼ê¸°
        yield buf
    elif not seen and buf:
        # assistantfinalì´ í•œ ë²ˆë„ ì•ˆ ë‚˜ì˜¨ ê²½ìš° fallback:
        # ì „ì²´ ë²„í¼ë¥¼ ê·¸ëƒ¥ ë³´ë‚´ê±°ë‚˜, ì •ì±…ì— ë”°ë¼ ë²„ë¦´ ìˆ˜ë„ ìˆìŒ.
        logger.warning(
            "[gpt-oss] assistantfinal ë§ˆì»¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì „ì²´ ë²„í¼ë¥¼ ê·¸ëŒ€ë¡œ ì „ì†¡í•©ë‹ˆë‹¤."
        )
        yield buf

def get_triton_client() -> InferenceServerClient:
    """Triton Client Singleton"""
    global _triton_client
    if _triton_client is None:
        try:
            _triton_client = InferenceServerClient(url=TRITON_URL, verbose=False)
            logger.info(f"âœ… Triton Client connected to {TRITON_URL}")
        except Exception as e:
            logger.error(f"âŒ Triton Client connection failed: {e}")
            raise e
    return _triton_client

def _make_inputs(
        prompt: str,
        *,
        max_tokens: int,
        temperature: float,
        top_p: float,
        stream: bool,
):
    text = InferInput("text_input", [1], "BYTES")
    text.set_data_from_numpy(np.array([prompt.encode("utf-8")], dtype=object))

    # vLLM-backend ê¶Œì¥ ëª…ì¹­: sampling_parameters
    sparams = InferInput("sampling_parameters", [1], "BYTES")

    # vLLM Python backend ìª½ êµ¬í˜„ì´ ë¬¸ìì—´ ê¸°ë°˜ íŒŒì‹±ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ
    params = {
        "temperature": str(float(temperature)),
        "top_p": str(float(top_p)),
        "max_tokens": str(int(max_tokens)),
        # stream í”Œë˜ê·¸ëŠ” ë³„ë„ BOOL ì¸í’‹ìœ¼ë¡œ ì „ë‹¬ ì¤‘
    }

    sparams.set_data_from_numpy(
        np.array([json.dumps(params).encode("utf-8")], dtype=object)
    )
    return text, sparams

def _triton_stream_generator(
        model_name: str,
        prompt: str,
        text: InferInput,
        sparams: InferInput,
        first_token_timeout: int = 10,
        idle_timeout: int = 20,
):
    cli = get_triton_client()  # Triton client ì¬ì‚¬ìš©

    stream_flag = InferInput("stream", [1], "BOOL")
    stream_flag.set_data_from_numpy(np.array([True], dtype=bool))
    outs = [InferRequestedOutput("text_output")]

    q: List[str] = []
    done = threading.Event()

    def on_resp(result, error):
        if error:
            logger.error(f"[ERR] Triton Callback Error: {error}")
            done.set()
            return

        if result is None:
            done.set()
            return

        arr = result.as_numpy("text_output")
        if arr is not None and len(arr) > 0:
            raw = arr[0]
            chunk = raw.decode("utf-8", errors="ignore")
            q.append(chunk)

        is_final = False
        try:
            resp = result.get_response()
            params = getattr(resp, "parameters", None)
            if params:
                flag = params.get("triton_final_response")
                if flag and getattr(flag, "bool_param", False):
                    is_final = True
        except Exception as e:
            logger.debug(f"[STREAM] triton_final_response check failed: {e}")

        if is_final:
            done.set()
            return

    # ìŠ¤íŠ¸ë¦¼ ì‹œì‘
    cli.start_stream(callback=on_resp)
    cli.async_stream_infer(model_name, inputs=[text, sparams, stream_flag], outputs=outs)

    try:
        start_time = time.time()
        last_yield_time = start_time
        got_first = False

        while not done.is_set() or q:
            if q:
                chunk = q.pop(0)
                got_first = True
                last_yield_time = time.time()
                yield chunk
            else:
                now = time.time()
                if not got_first and (now - start_time > first_token_timeout):
                    logger.warning("[WARN] First token timeout")
                    break
                if got_first and (now - last_yield_time > idle_timeout):
                    logger.warning("[WARN] Idle timeout after response started")
                    break
                time.sleep(0.005)
    finally:
        cli.stop_stream()

def _triton_infer_sync(
        model_name: str,
        prompt: str,
        *,
        max_tokens: int,
        temperature: float,
        top_p: float,
) -> str:
    # ğŸ”¹ ì—¬ê¸°ì„œ ë™ì ìœ¼ë¡œ max_new_tokens ê³„ì‚°
    dynamic_max_tokens = _compute_max_new_tokens(
        model_name=model_name,
        prompt=prompt,
        max_tokens_hint=max_tokens,
    )

    text, sparams = _make_inputs(
        prompt, max_tokens=dynamic_max_tokens,
        temperature=temperature, top_p=top_p, stream=True
    )

    accumulated_text = ""
    for chunk in _triton_stream_generator(model_name, prompt, text, sparams, 10, 20):
        accumulated_text += chunk

    accumulated_text = accumulated_text.strip()

    # gpt-oss ê³„ì—´ì´ë©´ assistantfinal ì´í›„ë§Œ ì¶”ì¶œ
    if _is_gpt_oss_model(model_name):
        return extract_final_answer(accumulated_text)

    return accumulated_text

def triton_infer(
        model_name: str,
        prompt: str,
        *,
        stream: bool = True,
        max_tokens: int = MAX_TOKENS,
        temperature: float = TEMPERATURE,
        top_p: float = TOP_P,
        timeout_first: int = 20,
        timeout_idle: int = 120,
):
    logger.info(f"[TRITON] infer start - model={model_name}, len={len(prompt)}")

    if stream:
        text, sparams = _make_inputs(
            prompt, max_tokens=max_tokens,
            temperature=temperature, top_p=top_p, stream=True
        )

        base_gen = _triton_stream_generator(
            model_name, prompt, text, sparams,
            timeout_first, timeout_idle
        )

        # gpt-oss ê³„ì—´ì´ë©´ assistantfinal ì´í›„ë§Œ ìŠ¤íŠ¸ë¦¬ë°
        if _is_gpt_oss_model(model_name):
            logger.info("[TRITON] gpt-oss ëª¨ë¸ ê°ì§€ â†’ assistantfinal ì´í›„ë§Œ ìŠ¤íŠ¸ë¦¬ë°")
            return stream_after_assistantfinal(base_gen)

        # ê·¸ ì™¸ ëª¨ë¸ì€ raw ìŠ¤íŠ¸ë¦¼ ê·¸ëŒ€ë¡œ
        return base_gen

    # Sync Path
    return _triton_infer_sync(
        model_name,
        prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
    )

def ensure_single_model_loaded(target_model: str, timeout: float = 120.0) -> None:
    """
    1) ë ˆí¬ì§€í† ë¦¬ ì¸ë±ìŠ¤ë¥¼ ë³´ê³  target ì´ì™¸ ëª¨ë¸ì€ ëª¨ë‘ unload
    2) target ëª¨ë¸ì´ READY ìƒíƒœê°€ ì•„ë‹ˆë©´ load + READY ë  ë•Œê¹Œì§€ ëŒ€ê¸°
    """
    cli = get_triton_client()

    # 1. ëª¨ë¸ ë ˆí¬ì§€í† ë¦¬ ì¸ë±ìŠ¤ ì¡°íšŒ
    try:
        repo = cli.get_model_repository_index()
    except Exception as e:
        logger.error(f"[TRITON] get_model_repository_index failed: {e}")
        raise

    # 2. target ì™¸ ëª¨ë¸ unload
    for m in getattr(repo, "models", []):
        name = getattr(m, "name", None)
        if not name or name == target_model:
            continue
        try:
            if cli.is_model_ready(name):
                logger.info(f"[TRITON] unloading other model: {name}")
                cli.unload_model(name)
        except Exception as e:
            logger.warning(f"[TRITON] unload_model({name}) failed: {e}")

    # 3. target ì´ ì´ë¯¸ READYë©´ ë°”ë¡œ ë¦¬í„´
    try:
        if cli.is_model_ready(target_model):
            logger.info(f"[TRITON] target model {target_model} already READY")
            return
    except Exception as e:
        logger.warning(f"[TRITON] is_model_ready({target_model}) error: {e}")

    # 4. target load
    logger.info(f"[TRITON] loading model: {target_model}")
    cli.load_model(target_model)

    # 5. READY ë  ë•Œê¹Œì§€ polling
    start = time.time()
    while True:
        try:
            if cli.is_model_ready(target_model):
                logger.info(f"[TRITON] model {target_model} READY")
                return
        except Exception as e:
            logger.warning(f"[TRITON] is_model_ready({target_model}) check failed: {e}")

        if time.time() - start > timeout:
            raise TimeoutError(f"Timeout while waiting for model {target_model} to be READY")

        time.sleep(0.5)

def unload_model_safe(model_name: str) -> None:
    cli = get_triton_client()
    try:
        if cli.is_model_ready(model_name):
            logger.info(f"[TRITON] unloading model: {model_name}")
            cli.unload_model(model_name)
    except Exception as e:
        logger.warning(f"[TRITON] unload_model({model_name}) failed: {e}")
