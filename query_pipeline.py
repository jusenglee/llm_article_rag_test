# 03_query_pipeline.py
# pip install "tritonclient[grpc]" qdrant-client transformers numpy llama-index tqdm orjson rapidfuzz

import os, json, time, threading, math, re
import numpy as np
from typing import List, Dict, Any, Tuple
from collections import defaultdict, Counter

from tqdm import tqdm
from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchText

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core import StorageContext, VectorStoreIndex
from transformers import AutoTokenizer
from tritonclient.grpc import InferenceServerClient, InferInput, InferRequestedOutput

from rapidfuzz import fuzz

# ---------------------------
# í™˜ê²½ ì„¤ì •
# ---------------------------
QDRANT_HOST  = os.getenv("QDRANT_HOST", "211.241.177.73")  # ê¶Œì¥: host + grpc_port
QDRANT_URL   = os.getenv("QDRANT_URL", "http://211.241.177.73:6333")  # (ì˜ˆë¹„)
COLLECTION   = os.getenv("QDRANT_COLLECTION", "peS2o_rag")
EMBED_MODEL  = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")  # 1024-d
TRITON_URL   = os.getenv("TRITON_URL", "211.241.177.73:8001")
MODEL_NAME   = os.getenv("TRITON_MODEL", "gemma_vllm_0")
TOKENIZER_ID = os.getenv("TOKENIZER_ID", "./")

TOP_K_BASE = 20
TOP_K_RETURN = 20
MAX_TOKENS    = 1024
TEMPERATURE   = 0.6
TOP_P         = 0.9

SCORE_THRESHOLD = 0.15
FUZZ_MIN        = 40

CTX_TOKEN_BUDGET = 2200
SNIPPET_MAX_CHARS = 1800

# ---------------------------
# Triton LLM
# ---------------------------
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_ID, trust_remote_code=True)

def triton_infer(prompt: str, stream=False) -> str:
    cli = InferenceServerClient(url=TRITON_URL, verbose=False)
    if not cli.is_model_ready(MODEL_NAME):
        raise RuntimeError(f"Triton model not ready: {MODEL_NAME}")

    text = InferInput("text_input", [1], "BYTES")
    text.set_data_from_numpy(np.array([prompt.encode("utf-8")], dtype=object))

    sparams = InferInput("sampling_parameters", [1], "BYTES")
    sparams.set_data_from_numpy(np.array([
        json.dumps({"temperature": TEMPERATURE, "top_p": TOP_P, "max_tokens": MAX_TOKENS}).encode("utf-8")
    ], dtype=object))

    stream_flag = InferInput("stream", [1], "BOOL")
    stream_flag.set_data_from_numpy(np.array([stream], dtype=bool))
    outs = [InferRequestedOutput("text_output")]

    done = threading.Event()
    acc_text = []

    def on_resp(result, error):
        if error:
            print("[ERR]", error)
            done.set(); return
        txt = result.as_numpy("text_output")[0].decode("utf-8")
        acc_text.append(txt)
        done.set()

    cli.start_stream(callback=on_resp)
    cli.async_stream_infer(MODEL_NAME, inputs=[text, sparams, stream_flag], outputs=outs)
    done.wait(timeout=180)
    cli.stop_stream()
    return "".join(acc_text).strip()

# ---------------------------
# RAG ë¹Œë“œ
# ---------------------------
def build_rag_objects():
    qdr = QdrantClient(host=QDRANT_HOST, grpc_port=6334, prefer_grpc=True)
    emb = HuggingFaceEmbedding(model_name=EMBED_MODEL, device="cuda", embed_batch_size=128, trust_remote_code=True)
    vstore = QdrantVectorStore(client=qdr, collection_name=COLLECTION)
    sctx = StorageContext.from_defaults(vector_store=vstore)
    index = VectorStoreIndex.from_vector_store(vector_store=vstore, embed_model=emb)
    retriever = index.as_retriever(similarity_top_k=TOP_K_BASE)
    return qdr, emb, retriever

# ---------------------------
# ì§ˆì˜ í™•ì¥
# ---------------------------
def dynamic_expand_query_llm(query: str) -> List[str]:
    prompt = f"""
You are a scientific keyword generator for academic search.
Respond ONLY with a JSON array of 8 concise English keywords.
Do NOT include explanations, examples, or formatting outside the array.

Input: {query}
Output:
    """

    resp = triton_infer(prompt).strip()

    #JSON ë°°ì—´ ë¶€ë¶„ë§Œ ì¶”ì¶œ (lazy match)
    match = re.search(r"\[[^\]]*\]", resp, re.S)
    if match:
        json_text = match.group(0)
        try:
            kws = json.loads(json_text)
            return [k.strip() for k in kws if isinstance(k, str) and k.strip()][:10]
        except json.JSONDecodeError:
            pass

    # fallback: ì‰¼í‘œ ê¸°ë°˜ íŒŒì‹±
    parts = re.split(r"[,;/\n]", resp)
    kws = [re.sub(r"[^A-Za-z0-9\s\-]", "", p).strip() for p in parts]
    kws = [k for k in kws if 2 <= len(k) <= 40 and re.search(r"[A-Za-z]", k)]
    return sorted(set(kws))[:10]

def expand_query_kor(query: str) -> Tuple[str, List[str]]:
    terms = dynamic_expand_query_llm(query)
    expanded = " OR ".join(sorted(set(terms or [query])))
    return expanded, sorted(set(terms))


# ---------------------------
# ê²€ìƒ‰ + ì¬ë­í‚¹
# ---------------------------
def _safe_query_embedding(emb, text: str):
    try:
        vec = emb.get_query_embedding(text)
    except Exception:
        vec = emb.get_text_embedding(text)
    v = np.asarray(vec, dtype=np.float32)
    n = np.linalg.norm(v) + 1e-12
    return (v / n).tolist()

def dense_retrieve_hybrid(client: QdrantClient, emb, expanded_text: str, keywords: List[str], top_k=TOP_K_BASE):
    q_vec = _safe_query_embedding(emb, expanded_text)

    hits = client.query_points(
        collection_name=COLLECTION,
        query=q_vec,
        limit=top_k,
        with_payload=True,
    ).points

    return hits

def expand_variants(keywords: List[str]) -> List[str]:
    variants = set()
    for k in keywords:
        variants.add(k)
        if not k.endswith("s"):
            variants.add(k + "s")
        if k.endswith("y"):
            variants.add(k[:-1] + "ies")
    return sorted(variants)

# ---------------------------
# ë¶€ìŠ¤íŒ… + ì¬ë­í‚¹
# ---------------------------
def _payload_texts(payload: Dict[str, Any]) -> Tuple[str, str]:
    node_json = payload.get("_node_text")
    body, title = "", payload.get("_title", "")
    if node_json:
        try:
            node = json.loads(node_json)
            body = node.get("text", "") or ""
            if not title:
                title = node.get("metadata", {}).get("title", "") or ""
        except Exception:
            pass
    body2 = payload.get("_node_text", "")
    if body2 and len(body2) > len(body):
        body = body2
    return body, title

def _keyword_score_for_hit(payload: Dict[str, Any], keywords: List[str]) -> float:
    body, title = _payload_texts(payload)
    if not body and not title:
        return 0.0
    best = 0.0
    for kw in keywords:
        if not kw:
            continue
        if title:
            s = fuzz.partial_ratio(kw, title)
            if s >= FUZZ_MIN:
                best = max(best, s * 1.2)
        if body:
            s = fuzz.partial_ratio(kw, body)
            if s >= FUZZ_MIN:
                best = max(best, s)
    return best / 200.0

def keyword_boost(hits, keywords: List[str]) -> Dict[str, float]:
    boost = {}
    for h in hits:
        try:
            b = _keyword_score_for_hit(h.payload or {}, keywords)
        except Exception:
            b = 0.0
        boost[h.id] = b
    return boost

def rrf_rerank(hits, boost_map: Dict[str, float], k=60):
    scored, id2hit = {}, {}
    for rank, h in enumerate(hits, start=1):
        id2hit[h.id] = h
        base = 1.0 / (k + rank)
        qdr = float(getattr(h, "score", 0.0) or 0.0)
        boost = boost_map.get(h.id, 0.0)
        scored[h.id] = scored.get(h.id, 0.0) + base + (qdr * 0.15) + boost
    reranked = sorted(scored.items(), key=lambda x: x[1], reverse=True)
    return [id2hit[i] for i, _ in reranked]

def dedup_by_doc(hits, max_k=TOP_K_RETURN):
    seen, out = set(), []
    for h in hits:
        payload = h.payload or {}
        # âœ… paper_idë¥¼ ê¸°ë³¸ ì‹ë³„ìë¡œ ì‚¬ìš©
        doc_id = payload.get("paper_id") or payload.get("doc_id") or payload.get("document_id") or payload.get("ref_doc_id")
        if not doc_id:
            doc_id = h.id  # fallback
        if doc_id in seen:
            continue
        seen.add(doc_id)
        out.append(h)
        if len(out) >= max_k:
            break
    return out

# ---------------------------
# ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
# ---------------------------
def clamp_text(s: str, max_chars=SNIPPET_MAX_CHARS) -> str:
    s = re.sub(r"\s+", " ", s).strip()
    return s[:max_chars]

def build_context_and_refs(hits) -> Tuple[str, List[Tuple[int, str, str]]]:
    items, refs = [], []
    for i, h in enumerate(hits, start=1):
        payload = h.payload or {}
        text = ""
        title = payload.get("_title", "")
        pid = payload.get("paper_id") or payload.get("doc_id") or "unknown"

        # âœ… 1ï¸âƒ£ ê¸°ë³¸: _node_textë¥¼ ìš°ì„  ì‚¬ìš©
        if "_node_text" in payload and payload["_node_text"]:
            text = payload["_node_text"]

        # âœ… 2ï¸âƒ£ ì˜ˆì™¸ì ìœ¼ë¡œ _node_contentê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° (JSON êµ¬ì¡° ì§€ì›)
        elif "_node_content" in payload:
            try:
                node = json.loads(payload["_node_content"])
                text = node.get("text", "")
                if not title:
                    title = node.get("metadata", {}).get("title", "") or ""
            except Exception:
                pass

        # âœ… 3ï¸âƒ£ ì •ë¦¬ ë° í´ë¨í”„
        text = clamp_text(text, SNIPPET_MAX_CHARS)
        if not title:
            title = text[:50] + "..."

        items.append(f"[{i}] {title}\n{text}")
        refs.append((i, title.strip(), str(pid)))

    return "\n\n".join(items), refs

def token_len(s: str) -> int:
    try:
        return len(tokenizer.encode(s))
    except Exception:
        return math.ceil(len(s) / 3)

def trim_context_to_budget(ctx: str, budget=CTX_TOKEN_BUDGET) -> str:
    if token_len(ctx) <= budget:
        return ctx
    paras = ctx.split("\n\n")
    kept, total = [], 0
    for p in paras:
        t = token_len(p) + 2
        if total + t > budget:
            break
        kept.append(p)
        total += t
    return "\n\n".join(kept)

# ---------------------------
# í”„ë¡¬í”„íŠ¸ (RAG/ëŒ€í™” ëª¨ë“œ ë¶„ë¦¬)
# ---------------------------
def build_rag_prompt(context_text, query, refs):
    """
    refs: (ë²ˆí˜¸, ì œëª©, paper_id)
    """
    if refs:
        ref_lines = "\n".join([f"[{n}] {title} (ID={pid})" for n, title, pid in refs])
    else:
        ref_lines = "N/A"

    system_msg = (
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìë¥¼ ë³´ì¡°í•˜ëŠ” LLMì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ê·¼ê±°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. "
        "ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ 'ì œê³µëœ ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤'ë¼ê³ ë§Œ ë§í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”. "
        "ê°€ëŠ¥í•˜ë©´ ë¬¸ì¥ ë‚´ì— ê·¼ê±° ë²ˆí˜¸ ê°ì£¼ë¥¼ í‘œì‹œí•˜ì„¸ìš”."
    )
    user_msg = f"""ë‹¤ìŒì€ ê´€ë ¨ ë¬¸ì„œ ë°œì·Œì…ë‹ˆë‹¤(ë²ˆí˜¸=ì¶œì²˜):

{context_text}

(ì¶œì²˜ ë²ˆí˜¸ ë§¤í•‘)
{ref_lines}

ì§ˆë¬¸: {query}

ìš”êµ¬ì‚¬í•­:
- ë¬¸ì¥ ë‚´ì— [1], [2] í˜•íƒœë¡œ ê·¼ê±° ë²ˆí˜¸ë¥¼ ë‹¬ì•„ì£¼ì„¸ìš”.
- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì“°ì§€ ë§ˆì„¸ìš”(ì¶”ê°€ ì§€ì‹ ê¸ˆì§€).
- ë§ˆì§€ë§‰ ì¤„ì— 'ì°¸ê³ ë¬¸í—Œ:' ë’¤ì— ë…¼ë¬¸ ì œëª©ì„ í•¨ê»˜ ë‚˜ì—´í•˜ì„¸ìš”. ì˜ˆì‹œ: ì°¸ê³ ë¬¸í—Œ: [1] ì œëª©A, [2] ì œëª©B
"""
    try:
        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ]

        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    except Exception:
        return f"<|system|>\n{system_msg}\n</s>\n<|user|>\n{user_msg}\n</s>\n<|assistant|>\n"

def build_chat_prompt(query: str) -> str:
    sys = "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ê°„ê²°í•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì¼ìƒ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µí•˜ì„¸ìš”."
    try:
        messages = [
            {"role": "system", "content": sys},
            {"role": "user", "content": query},
        ]
        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    except Exception:
        return f"<|system|>\n{sys}\n</s>\n<|user|>\n{query}\n</s>\n<|assistant|>\n"

def should_use_rag(query: str, hits, kw_list: List[str]) -> bool:
    if not hits:
        return False

    # ì ìˆ˜ ì²´í¬
    max_score = max([float(getattr(h, "score", 0.0) or 0.0) for h in hits])
    if max_score < SCORE_THRESHOLD:
        return False

    # ì§ˆì˜ ìœ í˜• ì²´í¬ (ë‹¨ìˆœ ëŒ€í™” ë°°ì œ)
    casual_patterns = [
        "ë‚ ì”¨", "ê¸°ë¶„", "ì•ˆë…•", "ì¢‹ì•„", "ì´ë¦„", "ëª‡ ì‹œ", "ëˆ„êµ¬", "ì‹¬ì‹¬", "ë°°ê³ íŒŒ",
        "ì˜¤ëŠ˜", "ì–´ë•Œ", "ã…‹ã…‹", "ã…", "??", "ì˜ì", "ì‚¬ë‘", "ê³ ë§ˆì›Œ", "ã…ã…‡"
    ]

    if any(re.search(re.escape(p), query) for p in casual_patterns):
        return False

    # í‚¤ì›Œë“œ í’ˆì§ˆ ì²´í¬ (ì˜ë¬¸ í•™ìˆ  í‚¤ì›Œë“œ ë¹„ìœ¨)
    english_ratio = sum(1 for k in kw_list if re.search(r"[A-Za-z]", k)) / (len(kw_list) or 1)
    if english_ratio < 0.4:
        return False

    return True

def decide_rag_needed(query: str) -> bool:
    prompt = f"""
    You are a controller that decides whether to use RAG (vector database search).
    If the user asks for factual, technical, or academic information, return "RAG".
    If the user asks for casual talk or opinion, return "CHAT".
    Only output one word: RAG or CHAT.

    User query: {query}
    Output:
    """
    resp = triton_infer(prompt).strip().upper()
    return "RAG" in resp

def rag_gate_decision(query: str, hits, kw_list: List[str], need_rag: bool) -> tuple[bool, str]:
    # íœ´ë¦¬ìŠ¤í‹± í•„í„° (ìŠ¤ì½”ì–´, ì¼ë°˜ ëŒ€í™” ê°ì§€ ë“±)
    gate_ok = should_use_rag(query, hits, kw_list)
    msg = ""

    # ê²°ê³¼ ë¡œê·¸
    if not hits:
        msg = "âŒ RAG ì‹œë„í–ˆìœ¼ë‚˜ ê²°ê³¼ ì—†ìŒ â†’ fallback to chat."
        print(msg)
        return False, msg
    elif max(float(h.score or 0.0) for h in hits) < SCORE_THRESHOLD:
        msg = "âš ï¸ ê²€ìƒ‰ ìŠ¤ì½”ì–´ ë‚®ìŒ â†’ fallback to chat."
        print(msg)
        return False, msg
    elif not (need_rag and gate_ok):
        msg = "ğŸ¤– ê²Œì´íŠ¸ íŒë‹¨ ê²°ê³¼: ì¼ë°˜ ëŒ€í™” ëª¨ë“œ ìœ ì§€."
        print(msg)
        return False, msg

    msg = "âœ… ê²Œì´íŠ¸ íŒë‹¨ ê²°ê³¼: RAG ê²€ìƒ‰/ì‘ë‹µ ìˆ˜í–‰."
    print(msg)
    return True, msg

# ---------------------------
# ë©”ì¸ ë£¨í”„
# ---------------------------
def main():
    qdr, emb, retriever = build_rag_objects()
    print("âœ… LLM-decides-RAG pipeline ready\n")

    while True:
        query = input("ì§ˆë¬¸ > ").strip()
        if not query or query.lower() in {"exit", "quit"}:
            break

        # LLMì´ íŒë‹¨
        need_rag = decide_rag_needed(query)
        print(f"ğŸ§­ LLM íŒë‹¨ ê²°ê³¼: {'RAG ê²€ìƒ‰ ìˆ˜í–‰' if need_rag else 'ì¼ë°˜ ëŒ€í™”'}")

        # RAG ê²€ìƒ‰
        expanded_text, kw_list = expand_query_kor(query)
        keywords = expand_variants(kw_list)
        print(keywords);
        hits = dense_retrieve_hybrid(qdr, emb, expanded_text, keywords, top_k=TOP_K_BASE)

        #  ê²Œì´íŠ¸ íŒë‹¨
        if not rag_gate_decision(query, hits, kw_list, need_rag):
            chat_prompt = build_chat_prompt(query)
            answer = triton_infer(chat_prompt, stream=True)
            print("\nğŸ“˜ ë‹µë³€:"); print(answer.strip()); print("-" * 80)
            continue

        # í†µê³¼ ì‹œ RAG ìˆ˜í–‰
        boost_map = keyword_boost(hits, kw_list)
        reranked = rrf_rerank(hits, boost_map, k=60)
        final_hits = dedup_by_doc(reranked, max_k=TOP_K_RETURN)

        ctx, refs = build_context_and_refs(final_hits)
        ctx = trim_context_to_budget(ctx, budget=CTX_TOKEN_BUDGET)

        print(f"\nğŸ” Retrieved top-{len(final_hits)} (after RRF+dedup).")
        rag_prompt = build_rag_prompt(ctx, query, refs)
        print("âš¡ LLM generating response...")
        answer = triton_infer(rag_prompt, stream=True)
        print("\nğŸ“˜ ë‹µë³€:"); print(answer.strip()); print("-" * 80)

if __name__ == "__main__":
    main()
