import os
import uuid
from pathlib import Path
import orjson
from typing import Iterator, Dict, Any, List
from tqdm import tqdm
from pymilvus import (
    connections,
    FieldSchema, CollectionSchema, DataType, Collection, utility
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Document
from llama_index.core.node_parser import SentenceSplitter

# ---------------------------
# ì„¤ì •
# ---------------------------
MILVUS_HOST = os.getenv("MILVUS_HOST", "localhost")
MILVUS_PORT = os.getenv("MILVUS_PORT", "19530")
COLLECTION = os.getenv("MILVUS_COLLECTION", "peS2o_rag")
EMBED_MODEL = os.getenv("EMBEDDING_MODEL", "BAAI/bge-large-en-v1.5")

JSON_PATH = Path("../data/peS2o_sample.jsonl")
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 120
BATCH_SIZE = 256
RESUME_STATE = Path(".ingest_resume_peS2o.txt")

# ---------------------------
# ìœ í‹¸
# ---------------------------
def iter_jsonl(path: Path) -> Iterator[Dict[str, Any]]:
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                yield orjson.loads(line)

def load_resume_offset() -> int:
    if RESUME_STATE.exists():
        try:
            return int(RESUME_STATE.read_text().strip())
        except Exception:
            return 0
    return 0

def save_resume_offset(n: int) -> None:
    RESUME_STATE.write_text(str(n))

def to_chunks(rec: Dict[str, Any], splitter: SentenceSplitter) -> List[Document]:
    # 1) ìµœìš°ì„ : text í•„ë“œ
    text = (rec.get("text") or "").strip()

    # 2) ëŒ€ì•ˆ ê²½ë¡œ: title/abstract/sections/body_textì—ì„œ í•©ì„±
    if not text:
        title = (rec.get("title") or "").strip()
        abstract = (rec.get("abstract") or rec.get("paperAbstract") or "").strip()

        # sections: [{heading, text}] ë˜ëŠ” [{section/section_title, text}] ê°€ì •
        sections_txt = []
        secs = rec.get("sections") or rec.get("body_text") or rec.get("pdf_parse", {}).get("body_text") or []
        if isinstance(secs, list):
            for s in secs[:50]:  # ê³¼ë„í•œ ë³¸ë¬¸ ë°©ì§€
                if isinstance(s, dict):
                    st = (s.get("text") or "").strip()
                    if st:
                        sections_txt.append(st)

        # body í›„ë³´ (ì¼ë¶€ ë°ì´í„°ì…‹ì€ 'body'ë‚˜ 'content' ë“±ìœ¼ë¡œ ìˆì„ ìˆ˜ ìˆìŒ)
        body = (rec.get("body") or rec.get("content") or "").strip()

        parts = [title, abstract] + sections_txt + ([body] if body else [])
        text = "\n\n".join([p for p in parts if p])

    if not text:
        return []  # ì—¬ì „íˆ ë¹„ë©´ ìŠ¤í‚µ

    paper_id = rec.get("id") or rec.get("paper_id") or rec.get("uid") or ""
    source = rec.get("source", "peS2o")

    docs: List[Document] = []
    for chunk in splitter.split_text(text):
        docs.append(Document(text=chunk, metadata={"paper_id": str(paper_id), "source": source}))
    return docs


# ---------------------------
# ë©”ì¸
# ---------------------------
def main():
    assert JSON_PATH.exists(), f"ì…ë ¥ íŒŒì¼ ì—†ìŒ: {JSON_PATH}"

    # Milvus ì—°ê²°
    connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
    print(f"âœ… Connected to Milvus at {MILVUS_HOST}:{MILVUS_PORT}")

    # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸ / ìƒì„±
    if not utility.has_collection(COLLECTION):
        print(f"ğŸ†• Creating new collection: {COLLECTION}")

        fields = [
            FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=64),
            FieldSchema(name="paper_id", dtype=DataType.VARCHAR, max_length=32),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=32),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1024),
        ]
        schema = CollectionSchema(fields, description="RAG embeddings for peS2o papers")
        collection = Collection(name=COLLECTION, schema=schema)

        collection.create_index(
            field_name="embedding",
            index_params={
                "index_type": "HNSW",
                "metric_type": "COSINE",
                "params": {"M": 16, "efConstruction": 200},
            },
        )
        print("âœ… Index created.")
    else:
        print(f"âœ… Found collection: {COLLECTION}")
        collection = Collection(name=COLLECTION)

    collection.load()

    # ì„ë² ë”© ëª¨ë¸
    embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL, device="cuda")
    splitter = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)

    # ì¬ì‹œì‘ ì˜¤í”„ì…‹
    start_idx = load_resume_offset()
    print(f"â†©ï¸  Resume from line index: {start_idx}")

    batch_ids, batch_paper_ids, batch_sources, batch_texts, batch_embeddings = [], [], [], [], []
    total_chunks = 0

    try:
        total_lines = sum(1 for _ in JSON_PATH.open("r", encoding="utf-8"))
    except Exception:
        total_lines = None

    with tqdm(total=total_lines, desc="Ingesting", unit="line", disable=(total_lines is None)) as pbar:
        for i, rec in enumerate(iter_jsonl(JSON_PATH)):
            if i < start_idx:
                if total_lines:
                    pbar.update(1)
                continue

            print(f"[DEBUG] line={i}  keys={list(rec.keys())[:10]}")  # â‘  JSON êµ¬ì¡°
            docs = to_chunks(rec, splitter)
            print(f"[DEBUG] chunks={len(docs)}")                     # â‘¡ ì²­í¬ ìˆ˜

            for doc in docs:
                emb = embed_model.get_text_embedding(doc.text)
                uid = f"{rec.get('id') or rec.get('paper_id') or ''}_{uuid.uuid4().hex}"
                batch_ids.append(uid)
                batch_paper_ids.append(str(rec.get("id") or rec.get("paper_id") or ""))
                batch_sources.append(rec.get("source", "peS2o"))
                batch_texts.append(doc.text)
                batch_embeddings.append(emb)

            if len(batch_ids) >= BATCH_SIZE:
                print(f"[DEBUG] insert batch: {len(batch_ids)}")
                data = [
                    batch_ids,
                    batch_paper_ids,
                    batch_sources,
                    batch_texts,
                    batch_embeddings,
                ]
                collection.insert(data)
                total_chunks += len(batch_ids)
                batch_ids, batch_paper_ids, batch_sources, batch_texts, batch_embeddings = [], [], [], [], []
                save_resume_offset(i + 1)

            if total_lines:
                pbar.update(1)

        # ì”ì—¬ ì²˜ë¦¬
        if batch_ids:
            data = [
                batch_ids,
                batch_paper_ids,
                batch_sources,
                batch_texts,
                batch_embeddings,
            ]
            collection.insert(data)
            total_chunks += len(batch_ids)
            save_resume_offset(i + 1)

    collection.flush()
    print(f"âœ… Done. Indexed chunks: {total_chunks}")
    print(f"ğŸ” Collection: {COLLECTION} @ {MILVUS_HOST}:{MILVUS_PORT}")

if __name__ == "__main__":
    main()
