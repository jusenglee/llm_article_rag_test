import os, json, math, time, threading, uuid, random
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, Future

import numpy as np
from tqdm import tqdm
from qdrant_client import QdrantClient
from qdrant_client.http.models import VectorParams, Distance, PointStruct

# ì„ë² ë”©/í† í¬ë‚˜ì´ì €
import torch
from sentence_transformers import SentenceTransformer

# ë¬¸ì¥ ë¶„í• ê¸° (ê°„ê²°/ë¹ ë¦„)
try:
    from llama_index.core.node_parser import SentenceSplitter
    _HAS_LLAMA_SPLITTER = True
except Exception:
    _HAS_LLAMA_SPLITTER = False

# =========================
# ì„¤ì •
# =========================
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_HTTP_PORT = int(os.getenv("QDRANT_HTTP_PORT", "6333"))
QDRANT_GRPC_PORT = int(os.getenv("QDRANT_GRPC_PORT", "6334"))
USE_GRPC = os.getenv("QDRANT_USE_GRPC", "0") == "1"   # gRPCê°€ ì—´ë ¤ìˆë‹¤ë©´ 1 ê¶Œì¥(ë„¤íŠ¸ì›Œí¬ ë ˆì´í„´ì‹œâ†“)

COLLECTION  = os.getenv("QDRANT_COLLECTION", "mistral_rag_100_000")

EMBED_MODEL = os.getenv("EMBED_MODEL", "../models/e5-mistral")
EMBED_BATCH = int(os.getenv("EMBED_BATCH", "512"))

CHUNK_SIZE     = int(os.getenv("CHUNK_SIZE", "512"))
CHUNK_OVERLAP  = int(os.getenv("CHUNK_OVERLAP", "120"))
JSON_PATH      = Path(os.getenv("JSON_PATH", "peS2o_sample.jsonl"))
RESUME_FILE    = Path(os.getenv("RESUME_FILE", ".ingest_resume_dualgpu.state"))

# Qdrant
DISTANCE_ENUM  = Distance.COSINE
ALLOW_RECREATE = os.getenv("ALLOW_RECREATE", "0") == "1"

# ì—…ì„œíŠ¸ ë³‘ë ¬ ìŠ¤ë ˆë“œ ìˆ˜
UPSERT_WORKERS = int(os.getenv("UPSERT_WORKERS", "2"))
# í•œ ë²ˆì— ì—…ì„œíŠ¸í•˜ëŠ” í¬ì¸íŠ¸ ìˆ˜
UPSERT_BATCH_SIZE = int(os.getenv("UPSERT_BATCH_SIZE", str(EMBED_BATCH)))

# tqdm ì—…ë°ì´íŠ¸ ë¹ˆë„ ì¤„ì´ê¸°
TQDM_MIN_INTERVAL = float(os.getenv("TQDM_MIN_INTERVAL", "0.3"))

# ë¬´ì‘ìœ„ ìƒ˜í”Œë§ ì„¤ì •: JSONL ë¼ì¸ ì¤‘ì—ì„œ ì´ ê°œìˆ˜ë§Œ ëœë¤ ì„ íƒ
SAMPLE_SIZE = int(os.getenv("SAMPLE_SIZE", "100000"))  #  10ë§Œê±´
SAMPLE_SEED = int(os.getenv("SAMPLE_SEED", "42"))

# =========================
# ìœ í‹¸
# =========================
def make_point_id(paper_id: str, chunk_idx: int, text: str) -> str:
    base = f"{paper_id}_{chunk_idx}_{text[:50]}"
    return str(uuid.uuid5(uuid.NAMESPACE_DNS, base))

def iter_jsonl_fast(path: Path, start_idx: int = 0):
    """ê°€ë²¼ìš´ JSONL ì´í„°ë ˆì´í„° (í‘œì¤€ json: C í™•ì¥ ê°€ì† + ì˜¤ë¸Œì íŠ¸ í’€ ìµœì†Œí™”)"""
    with path.open("r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i < start_idx:
                continue
            s = line.strip()
            if not s:
                continue
            yield i, json.loads(s)

def save_resume(n: int):
    RESUME_FILE.write_text(str(n), encoding="utf-8")

def load_resume() -> int:
    if RESUME_FILE.exists():
        try:
            return int(RESUME_FILE.read_text(encoding="utf-8").strip())
        except Exception:
            return 0
    return 0

# =========================
# ì„ë² ë” (1~2 GPU ìµœì í™”)
# =========================
class DualGPUEmbedder:
    """
    - SentenceTransformer ì§ì ‘ ì‚¬ìš©(ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”)
    - normalize_embeddings=True â†’ ì½”ì‚¬ì¸ ê¸¸ì´ ì •ê·œí™” ìë™
    - 2ê°œì˜ GPUê°€ ìˆìœ¼ë©´ í…ìŠ¤íŠ¸ ë°°ì¹˜ë¥¼ ë°˜ìœ¼ë¡œ ìª¼ê°œ ë‘ ëª¨ë¸ë¡œ ë™ì‹œ encode
    """
    def __init__(self, model_name: str, batch_size: int = 256, torch_dtype: Optional[torch.dtype] = None):
        self.batch_size = batch_size

        n_gpu = torch.cuda.device_count()
        self.devices = []
        if n_gpu >= 1:
            self.devices.append("cuda:0")
        if n_gpu >= 2:
            self.devices.append("cuda:1")

        # dtype ìµœì í™”(ì„ íƒ): fp16 ê¶Œì¥ (Ampere+)
        model_kwargs = {}
        if torch_dtype is not None:
            model_kwargs["torch_dtype"] = torch_dtype

        if not self.devices:
            # CPU fallback
            self.devices = ["cpu"]

        self.models: List[SentenceTransformer] = []
        for dev in self.devices:
            m = SentenceTransformer(model_name, device=dev)
            self.models.append(m)

        # ì„ë² ë”© ì°¨ì› ìë™ ê°ì§€
        self.embedding_dim = self.models[0].get_sentence_embedding_dimension()

    def encode_single(self, model: SentenceTransformer, texts: List[str]) -> np.ndarray:
        with torch.inference_mode():
            embs = model.encode(
                texts,
                batch_size=self.batch_size,
                convert_to_numpy=True,
                normalize_embeddings=True,  # ì½”ì‚¬ì¸ ì •ê·œí™” ì¼ê´„ ì²˜ë¦¬
                show_progress_bar=False,
            )
        return embs

    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        if len(self.models) == 1 or len(texts) == 0:
            embs = self.encode_single(self.models[0], texts)
            return embs.tolist()

        # 2ê°œ ì´ìƒ ëª¨ë¸ì¼ ë•Œ: ë°˜ìœ¼ë¡œ ìª¼ê°œ ë™ì‹œ ì²˜ë¦¬ (í˜„ì¬ 2GPUê¹Œì§€ ì‚¬ìš©)
        mid = len(texts) // 2
        parts = [texts[:mid], texts[mid:]]

        res: List[Optional[np.ndarray]] = [None] * len(self.models)

        def run(idx: int, subset: List[str]):
            if subset:
                res[idx] = self.encode_single(self.models[idx], subset)
            else:
                res[idx] = np.empty((0, self.embedding_dim), dtype=np.float32)

        threads = []
        for i in range(len(self.models)):
            t = threading.Thread(target=run, args=(i, parts[i] if i < 2 else []))
            t.start()
            threads.append(t)
        for t in threads:
            t.join()

        out = np.concatenate([r for r in res if r is not None], axis=0)
        return out.tolist()

# =========================
# Qdrant
# =========================
def ensure_collection(client: QdrantClient, collection: str, vector_dim: int):
    exists = False
    try:
        info = client.get_collection(collection)
        exists = True
        params = info.config.params
        cur_size = params.vectors.size
        cur_dist = str(params.vectors.distance).lower()
        if cur_size != vector_dim or "cosine" not in cur_dist:
            msg = (f"âš ï¸ ì»¬ë ‰ì…˜ ì •ì˜ ë¶ˆì¼ì¹˜: size={cur_size}, distance={params.vectors.distance} "
                   f"(í•„ìš” size={vector_dim}, distance=Cosine)")
            print(msg)
            if not ALLOW_RECREATE:
                raise RuntimeError(msg + "  (ALLOW_RECREATE=1 í™˜ê²½ë³€ìˆ˜ë¡œ ì¬ìƒì„± í—ˆìš©)")
            print("â™»ï¸  ì¬ìƒì„± ì§„í–‰ (ë°ì´í„° ì‚­ì œë¨) ...")
            client.recreate_collection(
                collection_name=collection,
                vectors_config=VectorParams(size=vector_dim, distance=DISTANCE_ENUM),
            )
            if RESUME_FILE.exists():
                RESUME_FILE.unlink(missing_ok=True)
            print(f"âœ… Collection recreated: {collection} (Cosine/{vector_dim})")
        else:
            print(f"âœ… Found collection: {collection} (Cosine/{vector_dim})")
    except Exception:
        if not exists:
            print(f"â„¹ï¸ ì»¬ë ‰ì…˜ ë¯¸ì¡´ì¬ â†’ ìƒì„±: {collection}")
            client.recreate_collection(
                collection_name=collection,
                vectors_config=VectorParams(size=vector_dim, distance=DISTANCE_ENUM),
            )
            print(f"âœ… Created collection: {collection} (Cosine/{vector_dim})")
        else:
            raise

class AsyncUpserter:
    """
    ì—…ì„œíŠ¸ë¥¼ ë¹„ë™ê¸° ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬
    - GPUê°€ ë‹¤ìŒ ë°°ì¹˜ë¥¼ encode í•˜ëŠ” ë™ì•ˆ
    - ë„¤íŠ¸ì›Œí¬ ì—…ì„œíŠ¸ëŠ” ë™ì‹œì— ì§„í–‰
    """
    def __init__(self, client: QdrantClient, collection: str, max_workers: int = 4):
        self.client = client
        self.collection = collection
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.futures: List[Future] = []

    def submit(self, ids: List[str], vectors: List[List[float]], payloads: List[Dict[str, Any]]):
        points = [PointStruct(id=ids[i], vector=vectors[i], payload=payloads[i]) for i in range(len(ids))]
        fut = self.executor.submit(self.client.upsert, collection_name=self.collection, points=points)
        self.futures.append(fut)

    def drain(self):
        # ëª¨ë“  ì—…ì„œíŠ¸ ì™„ë£Œ ëŒ€ê¸° (ì˜ˆì™¸ ì „íŒŒ)
        for fut in self.futures:
            fut.result()
        self.futures.clear()

    def shutdown(self):
        self.drain()
        self.executor.shutdown(wait=True)

# =========================
# ë©”ì¸
# =========================
def main():
    # 1) ëª¨ë¸ ë¡œë“œ(ì°¨ì› ìë™ ê°ì§€)
    #    - fp16 ì‚¬ìš©ì„ ì›í•˜ë©´ torch_dtype=torch.float16 ì „ë‹¬(í™˜ê²½ê³¼ VRAM ì—¬ìœ ì— ë”°ë¼)
    dtype = torch.float16 if os.getenv("EMBED_DTYPE_FP16", "1") == "1" and torch.cuda.is_available() else None
    embedder = DualGPUEmbedder(EMBED_MODEL, batch_size=EMBED_BATCH, torch_dtype=dtype)
    VECTOR_DIM = embedder.embedding_dim
    print(f"ğŸ§  Loaded model: {EMBED_MODEL} | dim={VECTOR_DIM} | devices={embedder.devices}")

    # 2) Qdrant ì—°ê²°
    if USE_GRPC:
        client = QdrantClient(host=QDRANT_HOST, grpc_port=QDRANT_GRPC_PORT, prefer_grpc=True)
    else:
        client = QdrantClient(host=QDRANT_HOST, port=QDRANT_HTTP_PORT)
    ensure_collection(client, COLLECTION, VECTOR_DIM)

    # 3) JSONL ì´ ë¼ì¸ ìˆ˜ ê³„ì‚° (ìƒ˜í”Œë§ ë° ì§„í–‰ë¥ ì„ ìœ„í•´)
    try:
        total_lines = sum(1 for _ in JSON_PATH.open("r", encoding="utf-8"))
    except Exception:
        total_lines = None

    # 4) ë¬´ì‘ìœ„ ìƒ˜í”Œë§ ì¸ë±ìŠ¤ ê²°ì •
    selected_idx_set: Optional[set[int]] = None
    effective_sample_size = None

    if total_lines is not None and SAMPLE_SIZE > 0:
        if SAMPLE_SIZE >= total_lines:
            # ì „ì²´ ë¼ì¸ ìˆ˜ê°€ 10ë§Œë³´ë‹¤ ì ìœ¼ë©´ ê·¸ëƒ¥ ì „ë¶€ ì‚¬ìš©
            print(f"â„¹ï¸ total_lines={total_lines} <= SAMPLE_SIZE={SAMPLE_SIZE} â†’ ì „ì²´ ë¼ì¸ ì‚¬ìš©")
            effective_sample_size = total_lines
        else:
            rng = random.Random(SAMPLE_SEED)
            sampled_indices = rng.sample(range(total_lines), SAMPLE_SIZE)
            sampled_indices.sort()
            selected_idx_set = set(sampled_indices)
            effective_sample_size = SAMPLE_SIZE
            print(f"ğŸ¯ ë¬´ì‘ìœ„ ìƒ˜í”Œë§: total_lines={total_lines}, sample_size={SAMPLE_SIZE}, seed={SAMPLE_SEED}")
    else:
        # total_linesë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆê±°ë‚˜ SAMPLE_SIZE=0 ì´ë©´ ì „ì²´ ì‚¬ìš©
        effective_sample_size = total_lines
        if SAMPLE_SIZE > 0 and total_lines is None:
            print("âš ï¸ total_lines ê³„ì‚° ì‹¤íŒ¨ â†’ ëœë¤ ìƒ˜í”Œë§ ë¶ˆê°€, ì „ì²´ ë¼ì¸ sequential ingest")

    # 5) ì¬ê°œ ìœ„ì¹˜
    #    ëœë¤ ìƒ˜í”Œë§ ëª¨ë“œì—ì„œëŠ” resumeë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ìƒ˜í”Œ êµ¬ì„±ì´ ë§¤ë²ˆ ë‹¬ë¼ì§)
    if selected_idx_set is not None:
        start_line = 0
        print("â„¹ï¸ ëœë¤ ìƒ˜í”Œë§ ëª¨ë“œ â†’ RESUME_FILE ë¬´ì‹œ (í•­ìƒ ì²˜ìŒë¶€í„° ìŠ¤ìº”)")
    else:
        start_line = load_resume()
    print(f"â†©ï¸ Resume from line {start_line}")

    # 6) ë¶„ë¦¬ê¸° (ì²­í‚¹ í’ˆì§ˆ ê°•í™”)
    if _HAS_LLAMA_SPLITTER:
        splitter = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
        def split_func(txt: str) -> List[str]:
            return splitter.split_text(txt)
    else:
        # ê°„ë‹¨í•œ í´ë°±: ë‹¨ë½ ìš°ì„  â†’ ê¸¸ë©´ ê³ ì •ê¸¸ì´ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° (overlap ì ìš©)
        def split_func(txt: str, size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
            out: List[str] = []
            # overlapì´ ë„ˆë¬´ í¬ë©´ ì‚´ì§ ì¤„ì¸ë‹¤ (ë¬´í•œë£¨í”„ ë°©ì§€)
            if overlap >= size:
                overlap = size // 3

            # ë‘ ì¤„ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ rough paragraph split
            paragraphs = [p.strip() for p in txt.split("\n\n") if p.strip()]
            for para in paragraphs:
                n = len(para)
                if n <= size:
                    out.append(para)
                    continue
                i = 0
                while i < n:
                    j = min(n, i + size)
                    out.append(para[i:j])
                    if j == n:
                        break
                    i = j - overlap
            return out

    # 7) ì—…ì„œíŠ¸ ë¹„ë™ê¸° ì‹¤í–‰ê¸°
    upserter = AsyncUpserter(client, COLLECTION, max_workers=UPSERT_WORKERS)

    total_chunks = 0
    docs_ingested = 0  # JSON ë ˆì½”ë“œ ê¸°ì¤€
    ids: List[str] = []
    payloads: List[Dict[str, Any]] = []
    texts: List[str] = []

    # ì„ë² ë”© ì‹œê°„ ì¸¡ì •ìš©
    embed_time_total = 0.0
    ingest_start_ts = time.time()

    # tqdm ì§„í–‰ë¥  ë°”: ëœë¤ ìƒ˜í”Œë§ì´ë©´ SAMPLE_SIZE ê¸°ì¤€, ì•„ë‹ˆë©´ total_lines ê¸°ì¤€
    pbar_total = effective_sample_size
    pbar = tqdm(total=pbar_total, desc="Ingesting", ncols=100, mininterval=TQDM_MIN_INTERVAL)

    try:
        for i, rec in iter_jsonl_fast(JSON_PATH, start_idx=start_line):
            # ëœë¤ ìƒ˜í”Œë§ì¸ ê²½ìš°: ì„ íƒëœ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
            if selected_idx_set is not None:
                if i not in selected_idx_set:
                    continue

            raw_text = (rec.get("text") or rec.get("_node_text") or "").strip()
            if not raw_text:
                # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ë¬¸ì„œ ì¹´ìš´íŠ¸ì— í¬í•¨í•˜ì§€ ì•Šê³  ìŠ¤í‚µ
                continue

            paper_id = str(rec.get("paper_id") or rec.get("id") or rec.get("doc_id") or "unknown")
            source   = rec.get("source", "peS2o")

            chunks = split_func(raw_text)
            if not chunks:
                continue

            for ci, chunk in enumerate(chunks):
                pid = make_point_id(paper_id, ci, chunk)
                ids.append(pid)
                payloads.append({
                    "paper_id": paper_id,
                    "source": source,
                    "_node_text": chunk,
                    "line_idx": i,
                    "chunk_idx": ci,
                })
                texts.append(chunk)

                # ì—…ì„œíŠ¸ ë‹¨ìœ„ëŠ” ì„ë² ë”© ë°°ì¹˜ì™€ ë™ì¼í•˜ê²Œ ë§ì¶¤
                if len(texts) >= UPSERT_BATCH_SIZE:
                    t0 = time.time()
                    vecs = embedder.embed_batch(texts)
                    embed_time_total += (time.time() - t0)

                    upserter.submit(ids, vecs, payloads)
                    total_chunks += len(texts)
                    ids.clear(); payloads.clear(); texts.clear()

                    # ëœë¤ ìƒ˜í”Œë§ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ resume ì €ì¥
                    if selected_idx_set is None:
                        save_resume(i + 1)

            docs_ingested += 1
            if pbar_total is not None:
                pbar.update(1)

        # ì”ì—¬ ì²˜ë¦¬
        if texts:
            t0 = time.time()
            vecs = embedder.embed_batch(texts)
            embed_time_total += (time.time() - t0)

            upserter.submit(ids, vecs, payloads)
            total_chunks += len(texts)

        # ëª¨ë“  ì—…ì„œíŠ¸ ì™„ë£Œ ëŒ€ê¸°
        upserter.drain()

        ingest_end_ts = time.time()
        wall_time = ingest_end_ts - ingest_start_ts

        # ===== ì„ë² ë”© ì†ë„ ë¡œê¹… =====
        print("\n====== Embedding / Ingest Stats ======")
        print(f"ğŸ“š Docs ingested     : {docs_ingested}")
        print(f"ğŸ”¹ Chunks ingested   : {total_chunks}")
        print(f"â± Total wall time   : {wall_time:.2f} s (IO + upsert + embed í¬í•¨)")

        if embed_time_total > 0:
            chunks_per_sec = total_chunks / embed_time_total
            docs_per_sec = docs_ingested / embed_time_total if docs_ingested > 0 else 0.0
            print(f"ğŸ§ª Embedding time    : {embed_time_total:.2f} s (ìˆœìˆ˜ embed_batch ëˆ„ì )")
            print(f"âš¡ Embedding speed   : {chunks_per_sec:.1f} chunks/s, {docs_per_sec:.1f} docs/s")
        else:
            print("âš ï¸ Embedding time ì¸¡ì •ê°’ì´ 0ì…ë‹ˆë‹¤. (ì„ë² ë”© í˜¸ì¶œì´ ì—†ì—ˆê±°ë‚˜ ì˜¤ë¥˜)")

        print("======================================\n")
        print(f"âœ… Done. Total {docs_ingested} docs, {total_chunks} chunks ingested into '{COLLECTION}'.")

    finally:
        upserter.shutdown()
        pbar.close()

if __name__ == "__main__":
    main()
